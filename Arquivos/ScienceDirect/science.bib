% This file was created with JabRef 2.7.
% Encoding: MacRoman

@ARTICLE{Črepinšek200599,
  author = {Matej Črepinšek and Marjan Mernik and Barrett R. Bryant and Faizan
	Javed and Alan Sprague},
  title = {Inferring Context-Free Grammars for Domain-Specific Languages},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2005},
  volume = {141},
  pages = {99 - 116},
  number = {4},
  note = {<ce:title>Proceedings of the Fifth Workshop on Language Descriptions,
	Tools, and Applications (LDTA 2005)</ce:title> <xocs:full-name>Language
	Descriptions, Tools, and Applications 2005</xocs:full-name>},
  abstract = {In the area of programming languages, context-free grammars (CFGs)
	are of special importance since almost all programming languages
	employ CFG's in their design. Recent approaches to CFG induction
	are not able to infer context-free grammars for general-purpose programming
	languages. In this paper it is shown that syntax of a small domain-specific
	language can be inferred from positive and negative programs provided
	by domain experts. In our work we are using the genetic programming
	approach in grammatical inference. Grammar-specific heuristic operators
	and nonrandom construction of the initial population are proposed
	to achieve this task. Suitability of the approach is shown by examples
	where underlying context-free grammars are successfully inferred.},
  doi = {10.1016/j.entcs.2005.02.055},
  issn = {1571-0661},
  keywords = {Grammar induction},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066105051765}
}

@INCOLLECTION{Atkinson2007105,
  author = {Colin Atkinson and Thomas Kühne},
  title = {A Tour of Language Customization Concepts},
  publisher = {Elsevier},
  year = {2007},
  editor = {Marvin V. Zelkowitz},
  volume = {70},
  series = {Advances in Computers},
  pages = {105 - 161},
  abstract = {Although the UML is often perceived as a universal modeling language
	it was not designed to fulfill this goal. Rather, the primary goal
	of the designers was to provide a unified modeling language. Because
	the applicability of the core language is limited, the UML has always
	offered mechanisms to enable it to be adapted to the needs of different
	users and applications. Over the years, a wide variety of different
	language customization mechanisms have been proposed, ranging from
	stereotypes, profiles, and metamodeling to domain-specific languages.
	However, the relationships between these different approaches and
	the different capabilities and options that they provide to users
	have never been clearly elaborated. In this chapter we provide a
	tour of the most important language customization mechanisms and
	by means of a unified case study we compare and contrast their pros
	and cons. We also review the current “state of the art” and present
	our view of how the “domain-customized language” approach to
	software engineering can best be supported in the future.},
  doi = {10.1016/S0065-2458(06)70003-1},
  issn = {0065-2458},
  url = {http://www.sciencedirect.com/science/article/pii/S0065245806700031}
}

@ARTICLE{Baars201051,
  author = {Arthur Baars and S. Doaitse Swierstra and Marcos Viera},
  title = {Typed Transformations of Typed Grammars: The Left Corner Transform},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2010},
  volume = {253},
  pages = {51 - 64},
  number = {7},
  note = {<ce:title>Proceedings of the Ninth Workshop on Language Descriptions
	Tools and Applications (LDTA 2009)</ce:title>},
  abstract = {One of the questions which comes up when using embedded domain specific
	languages is to what extent we can analyze and transform embedded
	programs, as normally done in more conventional compilers. Special
	problems arise when the host language is strongly typed, and this
	host type system is used to type the embedded language. In this paper
	we describe how we can use a library, which was designed for constructing
	transformations of typed abstract syntax, in the removal of left
	recursion from a typed grammar description. The algorithm we describe
	is the Left-Corner Transform, which is small enough to be fully explained,
	involved enough to be interesting, and complete enough to serve as
	a tutorial on how to proceed in similar cases. The described transformation
	has been successfully used in constructing a compositional and efficient
	alternative to the standard Haskell read function.},
  doi = {10.1016/j.entcs.2010.08.031},
  issn = {1571-0661},
  keywords = {GADT},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066110001106}
}

@ARTICLE{Bafna2008730,
  author = {Sapna Bafna and Julian Humphries and Daniel P. Miranker},
  title = {Schema driven assignment and implementation of life science identifiers
	(LSIDs)},
  journal = {Journal of Biomedical Informatics},
  year = {2008},
  volume = {41},
  pages = {730 - 738},
  number = {5},
  note = {<ce:title>Semantic Mashup of Biomedical Data</ce:title>},
  abstract = {Life science identifier (LSID) is a global unique identifier standard
	intended to help rationalize the unique archival requirements of
	biological data. We describe LSID implementation architecture such
	that data managed by a relational database management system may
	be integrated with the LSID protocol as an add-on layer. The approach
	requires a database administrator (DBA) to specify an export schema
	detailing the content and structure of the archived data, and a mapping
	of the existing database to that schema. This specification can be
	expressed using SQL view syntax. In effect, we define a SQL-like
	language for implementing LSIDs. We describe the mapping of the view
	definition to an implementation as a set of databases triggers and
	a fixed runtime library. Thus a compiler for a domain-specific language
	could be written that would reduce the implementation of LSIDs to
	the task of writing SQL view-like definitions.},
  doi = {10.1016/j.jbi.2008.05.014},
  issn = {1532-0464},
  keywords = {LSID},
  url = {http://www.sciencedirect.com/science/article/pii/S1532046408000816}
}

@ARTICLE{Balasubramanian2007171,
  author = {Krishnakumar Balasubramanian and Jaiganesh Balasubramanian and Jeff
	Parsons and Aniruddha Gokhale and Douglas C. Schmidt},
  title = {A Platform-Independent Component Modeling Language for Distributed
	Real-time and Embedded Systems},
  journal = {Journal of Computer and System Sciences},
  year = {2007},
  volume = {73},
  pages = {171 - 185},
  number = {2},
  note = {<ce:title>Special Issue: Real-time and Embedded Systems</ce:title>},
  abstract = {This paper provides two contributions to the study of developing and
	applying domain-specific modeling languages (DSMLS) to distributed
	real-time and embedded (DRE) systems—particularly those systems
	using standards-based QoS-enabled component middleware. First, it
	describes the Platform-Independent Component Modeling Language (PICML),
	which is a DSML that enables developers to define component interfaces,
	QoS parameters and software building rules, and also generates descriptor
	files that facilitate system deployment. Second, it applies PICML
	to an unmanned air vehicle (UAV) application portion of an emergency
	response system to show how PICML resolves key component-based DRE
	system development challenges. Our results show that the capabilities
	provided by PICML—combined with its design- and deployment-time
	validation capabilities—eliminates many common errors associated
	with conventional techniques, thereby increasing the effectiveness
	of applying QoS-enabled component middleware technologies to the
	DRE system domain.},
  doi = {10.1016/j.jcss.2006.04.008},
  issn = {0022-0000},
  keywords = {CoSMIC},
  url = {http://www.sciencedirect.com/science/article/pii/S002200000600050X}
}

@ARTICLE{Ballis200851,
  author = {D. Ballis and A. Baruzzo and M. Comini},
  title = {A Rule-based Method to Match Software Patterns Against UML Models},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2008},
  volume = {219},
  pages = {51 - 66},
  number = {0},
  note = {<ce:title>Proceedings of the Eighth International Workshop on Rule
	Based Programming (RULE 2007)</ce:title>},
  abstract = {In a UML model, different aspects of a system are covered by different
	types of diagrams and this bears the risk that an overall system
	specification becomes barely tractable by the designer. When the
	model grows, it is likely that the architectural integrity will be
	compromised by extensions and bug-fixing operations. Hence, it is
	important to provide means to help designers to search in big models
	for particular instances of some variable schema of UML models (design
	patterns) they construct. This can help them both to find potential
	problems in the architecture design and to ensure that intended architectural
	choices had not been broken by mistake. In this paper we propose
	a rule-based method to find matches of design patterns into a UML
	model. The method is general enough to tackle most patterns and antipatterns.},
  doi = {10.1016/j.entcs.2008.10.034},
  issn = {1571-0661},
  keywords = {Rule-based domain specific language},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066108004283}
}

@ARTICLE{vanBeijnum2010307,
  author = {Bert-Jan F. van Beijnum and Ing A. Widya and Enrico Marani},
  title = {Modeling the vagus nerve system with the Unified Modeling Language},
  journal = {Journal of Neuroscience Methods},
  year = {2010},
  volume = {193},
  pages = {307 - 320},
  number = {2},
  abstract = {Traditionally, the means of describing anatomical and physiological
	structures of the autonomic nervous system is natural language, drawings
	and images as represented in the scientific literature. In behavioral
	studies of this system, mathematical and electrical models and computer
	simulation tools are in use. In this article, we propose the use
	of the Unified Modeling Language to describe and specify the anatomical
	and physiological structures and indicate how these can be enriched
	to capture the behavioral view as well. Using the metamodel facilities
	of the language, we propose a domain specific language that captures
	the domain concepts, their relationships and constraints. Application
	of the language is demonstrated by modeling the vagus nerve in part.},
  doi = {10.1016/j.jneumeth.2010.08.015},
  issn = {0165-0270},
  keywords = {Vagus nerve},
  url = {http://www.sciencedirect.com/science/article/pii/S0165027010004759}
}

@ARTICLE{Bettini2011,
  author = {Lorenzo Bettini and Ferruccio Damiani and Ina Schaefer and Fabio
	Strocco},
  title = {TraitRecordJ: A programming language with traits and records},
  journal = {Science of Computer Programming},
  year = {2011},
  pages = { - },
  number = {0},
  abstract = {Traits have been designed as units for fine-grained reuse of behavior
	in the object-oriented paradigm. Records have been devised to complement
	traits for fine-grained reuse of state. In this paper, we present
	the language TraitRecordJ, a Java dialect with records and traits.
	Records and traits can be composed by explicit linguistic operations,
	allowing code manipulations to achieve fine-grained code reuse. Classes
	are assembled from (composite) records and traits and instantiated
	to generate objects. We introduce the language through examples and
	illustrate the prototypical implementation of TraitRecordJ using
	Xtext, an Eclipse framework for the development of programming languages
	as well as other domain-specific languages. Our implementation comprises
	an Eclipse-based editor for TraitRecordJ with typical IDE functionalities,
	and a stand-alone compiler, which translates TraitRecordJ programs
	into standard Java programs. As a case study, we present the TraitRecordJ
	implementation of a part of the software used in a web-based information
	system previously implemented in Java.},
  doi = {10.1016/j.scico.2011.06.007},
  issn = {0167-6423},
  keywords = {Java},
  url = {http://www.sciencedirect.com/science/article/pii/S0167642311001572}
}

@INCOLLECTION{Bilitchenko2011153,
  author = {Lesia Bilitchenko and Adam Liu and Douglas Densmore},
  title = {Chapter seven - The Eugene Language for Synthetic Biology},
  booktitle = {Synthetic Biology, Part B Computer Aided Design and DNA Assembly},
  publisher = {Academic Press},
  year = {2011},
  editor = {Christopher Voigt},
  volume = {498},
  series = {Methods in Enzymology},
  pages = {153 - 172},
  abstract = {Abstract Synthetic biological systems are currently created by an
	ad hoc, iterative process of design, simulation, and assembly. These
	systems would greatly benefit from the introduction of a more formalized
	and rigorous specification of the desired system components as well
	as constraints on their composition. In order to do so, the creation
	of robust and efficient design flows and tools is imperative. We
	present a human readable language (Eugene) which allows for both
	the specification of synthetic biological designs based on biological
	parts as well as providing a very expressive constraint system to
	drive the creation of composite devices from collection of parts.
	This chapter provides an overview of the language primitives as well
	as instructions on installation and use of Eugene v0.03b.},
  doi = {10.1016/B978-0-12-385120-8.00007-3},
  issn = {0076-6879},
  keywords = {Synthetic biology},
  url = {http://www.sciencedirect.com/science/article/pii/B9780123851208000073}
}

@ARTICLE{Bourdev2011243,
  author = {Lubomir Bourdev and Jaakko Järvi},
  title = {Efficient run-time dispatching in generic programming with minimal
	code bloat},
  journal = {Science of Computer Programming},
  year = {2011},
  volume = {76},
  pages = {243 - 257},
  number = {4},
  note = {<ce:title>Special issue on library-centric software design (LCSD
	2006)</ce:title>},
  abstract = {Generic programming with C++ templates results in efficient but inflexible
	code: efficient, because the exact types of inputs to generic functions
	are known at compile time; inflexible because they must be known
	at compile time. We show how to achieve run-time polymorphism without
	compromising performance by instantiating the generic algorithm with
	a comprehensive set of possible parameter types, and choosing the
	appropriate instantiation at run time. Applying this approach naïvely
	can result in excessive template bloat: a large number of template
	instantiations, many of which are identical at the assembly level.
	We show practical examples of this approach quickly approaching the
	limits of the compiler. Consequently, we combine this method of run-time
	polymorphism for generic programming, with a strategy for reducing
	the number of necessary template instantiations. We report on using
	our approach in GIL, Adobe’s open source Generic Image Library.
	We observed a notable reduction, up to 70% at times, in executable
	sizes of our test programs. This was the case even with compilers
	that perform aggressive template hoisting at the compiler level,
	due to significantly smaller dispatching code. The framework draws
	from both the generic and generative programming paradigms, using
	static metaprogramming to fine tune the compilation of a generic
	library. Our test bed, GIL, is deployed in a real world industrial
	setting, where code size is often an important factor.},
  doi = {10.1016/j.scico.2008.06.003},
  issn = {0167-6423},
  keywords = {Generic programming},
  url = {http://www.sciencedirect.com/science/article/pii/S0167642308000634}
}

@ARTICLE{Brabrand20072,
  author = {Claus Brabrand and Michael I. Schwartzbach},
  title = {The metafront system: Safe and extensible parsing and transformation},
  journal = {Science of Computer Programming},
  year = {2007},
  volume = {68},
  pages = {2 - 20},
  number = {1},
  note = {<ce:title>Special Issue on the ETAPS 2003 Workshop on Language Descriptions,
	Tools and Applications (LDTA ’03)</ce:title>},
  abstract = {We present the metafront tool for specifying flexible, safe, and efficient
	syntactic transformations between languages defined by context-free
	grammars. The transformations are guaranteed to terminate and to
	map grammatically legal input to grammatically legal output.
	
	We rely on a novel parser algorithm that is designed to support gradual
	extensions of a grammar by allowing productions to remain in a natural
	style and by statically reporting ambiguities and errors in terms
	of individual productions as they are being added.
	
	Our tool may be used as a parser generator in which the resulting
	parser automatically supports a flexible, safe, and efficient macro
	processor, or as an extensible lightweight compiler generator for
	domain-specific languages. We show substantial examples of both kinds.},
  doi = {10.1016/j.scico.2005.06.007},
  issn = {0167-6423},
  keywords = {Parsing},
  url = {http://www.sciencedirect.com/science/article/pii/S0167642307000779}
}

@ARTICLE{Brabrand2003592,
  author = {Claus Brabrand and Michael I. Schwartzbach and Mads Vanggaard},
  title = {The metafront System: Extensible Parsing and Transformation},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2003},
  volume = {82},
  pages = {592 - 611},
  number = {3},
  note = {<ce:title>LDTA'2003 - Language descriptions, Tools and Applications</ce:title>},
  abstract = {We present the metafront tool for specifying flexible, safe, and efficient
	syntactic transformations between languages defined by context-free
	grammars. The transformations are guaranteed to terminate and to
	map grammatically legal input to grammatically legal output.
	
	We rely on a novel parser algorithm that is designed to support gradual
	extensions of a grammar by allowing productions to remain in a natural
	style and by statically reporting ambiguities and errors in terms
	of individual productions as they are being added.
	
	Our tool may be used as a parser generator in which the resulting
	parser automatically supports a flexible, safe, and efficient macro
	processor, or as an extensible lightweight compiler generator for
	domain-specific languages. We show substantial examples of both kinds.},
  doi = {10.1016/S1571-0661(05)82630-1},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066105826301}
}

@ARTICLE{Braga2004149,
  author = {Christiano Braga and Alexandre Sztajnberg},
  title = {Towards a Rewriting Semantics for a Software Architecture Description
	Language},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2004},
  volume = {95},
  pages = {149 - 168},
  number = {0},
  note = {<ce:title>Proceedings of the Brazilian Workshop on Formal Methods</ce:title>},
  abstract = {Software architecture description languages (ADL) allow a software
	designer to focus on high- level aspects of an application by abstracting
	from the details of the components that compose an architecture.
	It is precisely this abstraction that makes ADLs suitable for verification
	using model checking techniques. ADLs are, in a way, domain-specific
	languages for aspects such as coordination, distribution and quality-of-service.
	The CBabel ADL defines the concept of contracts that precisely captures
	these architecture-level aspects. In this paper we propose a rewriting
	semantics for CBabel, that is, a formal semantics for CBabel specified
	in rewriting logic, a unifying formalism for concurrency models that
	has interesting properties as a logic and semantic framework due
	to its unified view of computation and proof. Using the Maude system,
	a high-performance implementation of rewriting logic, we formally
	verify the producer-consumer-buffer problem using model checking
	and state search.},
  doi = {10.1016/j.entcs.2004.04.010},
  issn = {1571-0661},
  keywords = {Rewriting semantics},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066104050194}
}

@ARTICLE{vandenBrand2005161,
  author = {M.G.J. van den Brand and B. Cornelissen and P.A. Olivier and J.J.
	Vinju},
  title = {TIDE: A Generic Debugging Framework — Tool Demonstration —},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2005},
  volume = {141},
  pages = {161 - 165},
  number = {4},
  note = {<ce:title>Proceedings of the Fifth Workshop on Language Descriptions,
	Tools, and Applications (LDTA 2005)</ce:title> <xocs:full-name>Language
	Descriptions, Tools, and Applications 2005</xocs:full-name>},
  abstract = {A language specific interactive debugger is one of the tools that
	we expect in any mature programming environment. We present applications
	of TIDE: a generic debugging framework that is related to the ASF+SDF
	Meta-Environment. TIDE can be applied to different levels of debugging
	that occur in language design.
	
	Firstly, TIDE was used to obtain a full-fledged debugger for language
	specifications based on term rewriting. Secondly, TIDE can be instantiated
	for any other programming language, including but not limited to
	domain specific languages that are defined and implemented using
	ASF+SDF.
	
	We demonstrate the common debugging interface, and indicate the amount
	of effort needed to instantiate new debuggers based on TIDE.},
  doi = {10.1016/j.entcs.2005.02.056},
  issn = {1571-0661},
  keywords = {Generic debugging},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066105051789}
}

@ARTICLE{vandenBrand2002144,
  author = {Mark van den Brand and Ralf Lämmel},
  title = {Preface: Volume 65, Issue 3},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2002},
  volume = {65},
  pages = {144 - 145},
  number = {3},
  note = {<ce:title>LDTA 2002, Second Workshop on Language Descriptions, Tools
	and Applications (Satellite Event of ETAPS 2002)</ce:title>},
  abstract = {Foreword This is the second edition of the Workshop on Language Descriptions,
	Tools, and Applications (LDTA). Theme of this workshop is the description
	of (programming) languages and the development and/or generation
	of tools for these languages based on formal descriptions. The aim
	of this workshop is to bring together researchers from academia and
	industry interested in the formal definition of languages, and the
	use of these definitions for the derivation of tools and software
	applications (e.g., interactive transformation tools, software renovation
	tools, application generators). The contributions for this workshop
	are based on all kinds and styles of formal language definition including
	attribute grammars, algebraic approaches, action semantics, operational
	semantics, and denotational semantics. Many aspects of the definition
	of languages, and the derivation of tools and applications are independent
	of the actual formalism used. The workshop also welcomes contributions
	to compare formalisms or to discuss the benefits and drawbacks of
	specific formalisms.
	
	The LDTA 2002 program consists of 6 regular peer-reviewed papers,
	which were selected from 14 submissions, one invited talk by Charles
	Consel on “Domain-Specific Languages: What, Why, How”, and 4
	invited tool demonstrations. The selected papers cover a broad range
	of themes like: applying rewriting strategies to build interpreters,
	program slicing based on XML, application of aspect oriented programming
	and XML, multi-lingual bibliography tooling, and incremental attribute
	evaluators. The tool demonstrations are concerned with presentations
	of the new ELAN environment, an environment for action semantics,
	the Grammar Deployment Kit (GDK), and the ELI system.
	
	This volume will be published as volume 65-3 in the series Electronic
	Notes in Theoretical Computer Science (ENTCS). This series is published
	electronically through the facilities of Elsevier Science B.V. and
	its auspices. The volumes in the ENTCS series can be accessed at
	the URL http://www.elsevier.nl/locate/entcs
	
	We would like to thank the program committee members for their help
	in evaluating the papers and making a scientifically interesting
	selection. Furthermore, we would like to thank the ETAPS organizing
	committee for taking care of the local organization of our workshop.
	We are very pleased that these proceedings are published in the Electronic
	Notes in Theoretical Computer Science (ENTCS) by Elsevier, which
	increases the visibility of the LDTA workshop.
	
	Organizing Committee 
	
	Program Committee},
  doi = {10.1016/S1571-0661(05)80433-5},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066105804335}
}

@ARTICLE{Broom2002449,
  author = {Bradley Broom and Rob Fowler and Ken Kennedy},
  title = {KelpIO: a telescope-ready domain-specific I/O library for irregular
	block-structured applications},
  journal = {Future Generation Computer Systems},
  year = {2002},
  volume = {18},
  pages = {449 - 460},
  number = {4},
  note = {<ce:title>Best papers from Symp. on Cluster Computing and the Grid
	(CCGrid2001)</ce:title>},
  abstract = {To ameliorate the need to spend significant programmer time modifying
	parallel programs to achieve high-performance, while maintaining
	compact, comprehensible source codes, this paper advocates the use
	of telescoping languages technology to automatically apply, during
	the normal compilation process, high-level performance enhancing
	transformations to applications using a high-level domain-specific
	I/O library. We believe that this approach will be more acceptable
	to application developers than new language extensions, but will
	be just as amenable to optimization by advanced compilers, effectively
	making it a domain-specific language extension for I/O.
	
	The paper describes a domain-specific I/O library for irregular block-structured
	applications based on the KeLP library, describes high-level transformations
	of the library primitives for improving performance, and describes
	how a high-level domain-specific optimizer for applying these transformations
	could be constructed using the telescoping languages framework.},
  doi = {10.1016/S0167-739X(01)00072-3},
  issn = {0167-739X},
  keywords = {Domain-specific languages},
  url = {http://www.sciencedirect.com/science/article/pii/S0167739X01000723}
}

@ARTICLE{Bézivin200669,
  author = {Jean Bézivin and Frédéric Jouault},
  title = {Using ATL for Checking Models},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2006},
  volume = {152},
  pages = {69 - 81},
  number = {0},
  note = {<ce:title>Proceedings of the International Workshop on Graph and
	Model Transformation (GraMoT 2005)</ce:title> <xocs:full-name>Graph
	and Model Transformation 2005</xocs:full-name>},
  abstract = {Working with models often requires the ability to assert the compliance
	of a given model to a given set of constraints. Some tools are able
	to check OCL invariants on UML models. However, there are very few
	tools able to do the same for any metamodel. This is quite penalizing
	for the DSL (Domain Specific Language) approach to model engineering.
	In this paper we propose a metamodel-independent solution to this
	problem that uses ATL (Atlas Transformation Language). This solution
	has been implemented as an Eclipse-based plugin.},
  doi = {10.1016/j.entcs.2006.01.015},
  issn = {1571-0661},
  keywords = {Model Engineering},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066106001393}
}

@ARTICLE{Campos200945,
  author = {Marco Devesas Campos and L.S. Barbosa},
  title = {Implementation of an Orchestration Language as a Haskell Domain Specific
	Language},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2009},
  volume = {255},
  pages = {45 - 64},
  number = {0},
  note = {<ce:title>Proceedings of the 8th International Workshop on the Foundations
	of Coordination Languages and Software Architectures (FOCLASA 2009)</ce:title>},
  abstract = {Even though concurrent programming has been a hot topic of discussion
	in Computer Science for the past 30 years, the community has yet
	to settle on a, or a few standard approaches to implement concurrent
	programs. But as more and more cores inhabit our CPUs and more and
	more services are made available on the web the problem of coordinating
	different tasks becomes increasingly relevant. The present paper
	addresses this problem with an implementation of the orchestration
	language Orc as a domain specific language in Haskell. Orc was, therefore,
	realized as a combinator library using the lightweight threads and
	the communication and synchronization primitives of the Concurrent
	Haskell library. With this implementation it becomes possible to
	create orchestrations that re-use existing Haskell code and, conversely,
	re-use orchestrations inside other Haskell programs. The complexity
	inherent to distributed computation, entails the need for the classification
	of efficient, reusable, concurrent programming patterns. The paper
	discusses how the calculus of recursive schemes used in the derivation
	of functional programs, scales up to a distributed setting. It is
	shown, in particular, how to parallelize the entire class of binary
	tree hylomorphisms.},
  doi = {10.1016/j.entcs.2009.10.024},
  issn = {1571-0661},
  keywords = {Orc},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066109004447}
}

@ARTICLE{Cao2005119,
  author = {Fei Cao and Barrett R. Bryant and Carol C. Burt and Rajeev R. Raje
	and Andrew M. Olson and Mikhail Auguston},
  title = {A Component Assembly Approach Based On Aspect-Oriented Generative
	Domain Modeling},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2005},
  volume = {114},
  pages = {119 - 136},
  number = {0},
  note = {<ce:title>Proceedings of the Software Composition Workshop (SC 2004)</ce:title>
	<xocs:full-name>Software Composition Workshop 2004</xocs:full-name>},
  abstract = {We present an approach towards automatic component assembly based
	on aspect-oriented generative domain modeling. It involves the lifecycle
	covering the component specification generation, and subsequent assembly
	of implementation components to produce the final software system.
	Aspect-oriented techniques are applied to capture the crosscutting
	concerns that emerge during the assembly process. Subsequently, those
	concerns are woven to generate glue/wrapper code for assembling heterogeneous
	components to construct a single integrated system.},
  doi = {10.1016/j.entcs.2004.02.070},
  issn = {1571-0661},
  keywords = {Component Assembly},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066104052697}
}

@ARTICLE{Charsel20021,
  author = {Charsel and Consel},
  title = {Domain-Specific Languages: What, Why, How},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2002},
  volume = {65},
  pages = {1 - },
  number = {3},
  note = {<ce:title>LDTA 2002, Second Workshop on Language Descriptions, Tools
	and Applications (Satellite Event of ETAPS 2002)</ce:title>},
  abstract = {Domain-specific languages (DSLs) are being increasingly used as a
	realistic approach to address a program family. That is, a set of
	programs that shares enough commonalities to be considered as a whole.
	These programs may already exist or be expected to be developed.
	In this situation, in principle, software development can benefit
	from introducing a DSL in that (1) it offers concise and specific
	notations to express a member of the program family, and (2) it enables
	the development of safe code thanks to its restricted semantics and/or
	requirements for additional information.
	
	The Compose group has developed DSLs for various domains such as device
	drivers, active networking, and process scheduling, and built some
	experience in designing and implementing DSLs. In this talk we will
	report on the outcomes of this line of work. In particular, we will
	attempt to provide a practical definition of a DSL, and give the
	conditions to make this approach successful. We will discuss actual
	benefits of the DSL approach. Finally, we will outline a methodology
	to design and implement a DSL.},
  doi = {10.1016/S1571-0661(04)80422-5},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066104804225}
}

@ARTICLE{Chavarriaga20091329,
  author = {Enrique Chavarriaga and José A. Macías},
  title = {A model-driven approach to building modern Semantic Web-Based User
	Interfaces},
  journal = {Advances in Engineering Software},
  year = {2009},
  volume = {40},
  pages = {1329 - 1334},
  number = {12},
  note = {<ce:title>Designing, modelling and implementing interactive systems</ce:title>},
  abstract = {The Semantic Web has widely spread in the last 10 years as a suitable
	web platform to support semantics and expressive information seeking.
	However, one of the main problems with this paradigm is still the
	representation and manipulation of ontologies as well as the complex
	relationships that they implicitly represent. Actually, this remains
	a challenge when unskilled users have to deal with this abstract
	representation in order to carry out daily solving-problem activities
	(e.g., designing web applications based on ontologies). This probably
	made the Semantic Web to decrease in popularity, also being commercially
	unsupported and overcame by recent technologies and services based
	on the Web 2.0, the emerging end-user-focused web concept. All in
	all, the specification of Model-Based User Interfaces fits very well
	to both paradigms. Accordingly, the aim of this work is to provide
	new ways of modeling user interfaces based on semantic models that
	better fit the domain problem. At the same time, we think of exploiting
	interactive features through current and modern end-user programming
	elements based on the Web 2.0, finally contributing to an architecture
	that supports higher interactive end-user interfaces on the web.},
  doi = {10.1016/j.advengsoft.2009.01.016},
  issn = {0965-9978},
  keywords = {User interfaces for the Semantic Web and Web 2.0},
  url = {http://www.sciencedirect.com/science/article/pii/S0965997809000416}
}

@ARTICLE{Clark201075,
  author = {Tony Clark and Laurence Tratt},
  title = {Formalizing Homogeneous Language Embeddings},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2010},
  volume = {253},
  pages = {75 - 88},
  number = {7},
  note = {<ce:title>Proceedings of the Ninth Workshop on Language Descriptions
	Tools and Applications (LDTA 2009)</ce:title>},
  abstract = {The cost of implementing syntactically distinct Domain Specific Languages
	(DSLs) can be reduced by homogeneously embedding them in a host language
	in cooperation with its compiler. Current homogeneous embedding approaches
	either restrict the embedding of multiple DSLs in order to provide
	safety guarantees, or allow multiple DSLs to be embedded but force
	the user to deal with the interoperability burden. In this paper
	we present the μ-calculus which allows parameterisable language
	embeddings to be specified and analysed. By reducing the problem
	to its core essentials we are able to show how multiple, expressive
	language embeddings can be defined in a homogeneous embedding context.
	We further show how variant calculi with safety guarantees can be
	defined.},
  doi = {10.1016/j.entcs.2010.08.033},
  issn = {1571-0661},
  keywords = {Domain specific languages},
  url = {http://www.sciencedirect.com/science/article/pii/S157106611000112X}
}

@ARTICLE{Cleaveland19981,
  author = {Rance Cleaveland and Michael Mislove and Philip Mulry},
  title = {Preface: Volume 14, Isuue 1},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {1998},
  volume = {14},
  pages = {1 - 2},
  number = {0},
  note = {<ce:title>US-Brazil Joint Workshops on the Formal Foundations of
	Software Systems</ce:title>},
  abstract = {US - Brazil Joint Workshops on the Formal Foundations of Software
	Systems Rio de Janeiro, May 5 - 9, 1997 and New Orleans, November
	13 - 16, 1997 The papers in this volume form the Proceedings of two
	workshops that took place in 1997. The first workshop took place
	in Rio de Janeiro in May, 1997, and the second took place in New
	Orleans the following November. Both meetings were sponsored jointly
	by the US National Science Foundation and its Brazilian counterpart,
	CNPq. The goal of the workshops was to foster collaborative research
	efforts between US and Brazilian researchers. These workshops marked
	the culmination of two years of talks, visits, and planning by the
	members of NSF and CNPq, and their respective constituencies. Out
	of the same series of exchanges sprang another workshop in Porto
	Alegre, Brazil on Robotics and Intelligent Systems. While many Brazilian
	students come to US for advanced studies, the majority still choose
	institutions in Western Europe; nonetheless, these workshops have
	uncovered a wealth of common interests and encouraged several new
	collaborations. Beyond the workshops, the contacts have facilitated
	additional joint ventures, such as the new NSF-CNPq Program for the
	CISE Directorate which has received a significant number of proposals
	from collaborators unrelated to the workshops.
	
	The workshops helped the participants build a foundation for such
	efforts by providing a forum where participants could learn about
	each others' research interests and begin to establish a basis for
	collaborative work. The topics of interest ranged from theoretical
	concepts and formalisms to techniques and tools for the systematic
	construction of software systems. The areas where research collaboration
	possibilities have emerged or are about to emerge include object-based
	programming, tools and specification techniques, functional programming
	and domain-specific languages, and basic theoretical areas ranging
	from domain theory and category theory to type theory and concurrency.
	
	More detailed information about the workshops can be accessed on line.
	The list of participants and other information about each workshop
	are available as follows. Information about the Rio workshop can
	be found at www.kestrel.edu/~jullig/rio97, and information about
	the following New Orleans workshop can be found at www.math.tulane.edu/usbrazil.html.
	Of particular interest may be the position statements the participants
	submitted that describe the areas where they expect collaboration
	may be forthcoming.
	
	As a result of the efforts of NSF and CNPq, programs have been established
	in both agencies specifically aimed at providing support for emerging
	collaborations between researchers in both countries. Information
	about these programs can be found on line at the NSF and CNPq web
	sites: www.nsf.gov and www.cnpq.br.
	
	The Organizers of the meetings wish to thank the participants who
	took part in the meetings, but most especially the representatives
	of NSF: Frank Anger and Rita Virginia Rodriguez, and the representatives
	of CNPq: Virgilio Almeida and Rosa Maria Viccari, all of whom provided
	financial support for the workshops. Without their encouragement
	and the initiative they fostered, neither the workshops nor the joint
	research support program would have taken place.},
  doi = {10.1016/S1571-0661(05)80225-7},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066105802257}
}

@ARTICLE{Coden2005422,
  author = {Anni R. Coden and Serguei V. Pakhomov and Rie K. Ando and Patrick
	H. Duffy and Christopher G. Chute},
  title = {Domain-specific language models and lexicons for tagging},
  journal = {Journal of Biomedical Informatics},
  year = {2005},
  volume = {38},
  pages = {422 - 430},
  number = {6},
  abstract = {Accurate and reliable part-of-speech tagging is useful for many Natural
	Language Processing (NLP) tasks that form the foundation of NLP-based
	approaches to information retrieval and data mining. In general,
	large annotated corpora are necessary to achieve desired part-of-speech
	tagger accuracy. We show that a large annotated general-English corpus
	is not sufficient for building a part-of-speech tagger model adequate
	for tagging documents from the medical domain. However, adding a
	quite small domain-specific corpus to a large general-English one
	boosts performance to over 92% accuracy from 87% in our studies.
	We also suggest a number of characteristics to quantify the similarities
	between a training corpus and the test data. These results give guidance
	for creating an appropriate corpus for building a part-of-speech
	tagger model that gives satisfactory accuracy results on a new domain
	at a relatively small cost.},
  doi = {10.1016/j.jbi.2005.02.009},
  issn = {1532-0464},
  keywords = {Clinical report analysis},
  url = {http://www.sciencedirect.com/science/article/pii/S1532046405000213}
}

@ARTICLE{Cohen200625,
  author = {Albert Cohen and Sébastien Donadio and Maria-Jesus Garzaran and
	Christoph Herrmann and Oleg Kiselyov and David Padua},
  title = {In search of a program generator to implement generic transformations
	for high-performance computing},
  journal = {Science of Computer Programming},
  year = {2006},
  volume = {62},
  pages = {25 - 46},
  number = {1},
  note = {<ce:title>Special Issue on the First MetaOCaml Workshop 2004</ce:title>},
  abstract = {The quality of compiler-optimized code for high-performance applications
	is far behind what optimization and domain experts can achieve by
	hand. Although it may seem surprising at first glance, the performance
	gap has been widening over time, due to the tremendous complexity
	increase in microprocessor and memory architectures, and to the rising
	level of abstraction of popular programming languages and styles.
	This paper explores in-between solutions, neither fully automatic
	nor fully manual ways to adapt a computationally intensive application
	to the target architecture. By mimicking complex sequences of transformations
	useful to optimize real codes, we show that generative programming
	is a practical means to implement architecture-aware optimizations
	for high-performance applications.
	
	This work explores the promises of generative programming languages
	and techniques for the high-performance computing expert. We show
	that complex, architecture-specific optimizations can be implemented
	in a type-safe, purely generative framework. Peak performance is
	achievable through the careful combination of a high-level, multi-stage
	evaluation language–MetaOCaml–with low-level code generation
	techniques. Nevertheless, our results also show that generative approaches
	for high-performance computing do not come without technical caveats
	and implementation barriers concerning productivity and reuse. We
	describe these difficulties and identify ways to hide or overcome
	them, from abstract syntaxes to heterogeneous generators of code
	generators, combining high-level and type-safe multi-stage programming
	with a back-end generator of imperative code.},
  doi = {10.1016/j.scico.2005.10.013},
  issn = {0167-6423},
  keywords = {Multi-stage programming},
  url = {http://www.sciencedirect.com/science/article/pii/S0167642306000724}
}

@ARTICLE{VanCutsem200980,
  author = {Tom Van Cutsem and Stijn Mostinckx and Wolfgang De Meuter},
  title = {Linguistic symbiosis between event loop actors and threads},
  journal = {Computer Languages, Systems \&amp; Structures},
  year = {2009},
  volume = {35},
  pages = {80 - 98},
  number = {1},
  note = {<ce:title>ESUG 2007 International Conference on Dynamic Languages
	(ESUG/ICDL 2007)</ce:title>},
  abstract = {In modern programming languages, concurrency control can be traced
	back to one of two different schools: actor-based message passing
	concurrency and thread-based shared-state concurrency. This paper
	describes a linguistic symbiosis between two programming languages
	with such different concurrency models. More specifically, we describe
	a novel symbiosis between actors represented as event loops on the
	one hand and threads on the other. This symbiosis ensures that the
	invariants of the actor-based concurrency model are not violated
	by engaging in symbiosis with multithreaded programs. The proposed
	mapping is validated by means of a concrete symbiosis between AmbientTalk,
	a flexible, domain-specific language for writing distributed programs
	and Java, a conventional object-oriented language. This symbiosis
	allows the domain-specific language to reuse existing software components
	written in a multithreaded language without sacrificing the beneficial
	event-driven properties of the actor concurrency model.},
  doi = {10.1016/j.cl.2008.06.005},
  issn = {1477-8424},
  keywords = {Actors},
  url = {http://www.sciencedirect.com/science/article/pii/S1477842408000249}
}

@ARTICLE{David2004265,
  author = {David and Wile},
  title = {Lessons learned from real DSL experiments},
  journal = {Science of Computer Programming},
  year = {2004},
  volume = {51},
  pages = {265 - 290},
  number = {3},
  abstract = {Over the years, our group, led by Bob Balzer, designed and implemented
	three domain-specific languages for use in real applications. Each
	was invented to “showcase” DSL language design and implementation
	technology that was the focus of our then-current research. Each
	of these was actually a prototype for what would have taken more
	time to engineer and polish before putting into practice. Although
	each effort was essentially successful, none of the languages was
	ever followed up with the subsequent engineering efforts that we
	expected or at least hoped for. Herein I elaborate where these language
	efforts succeeded and where they failed, gleaning lessons for others
	who take the somewhat risky step of committing to develop a DSL for
	a particular user community.},
  doi = {10.1016/j.scico.2003.12.006},
  issn = {0167-6423},
  keywords = {Domain-specific language},
  url = {http://www.sciencedirect.com/science/article/pii/S0167642304000310}
}

@ARTICLE{Degenne20093527,
  author = {P. Degenne and D. Lo Seen and D. Parigot and R. Forax and A. Tran
	and A. Ait Lahcen and O. Curé and R. Jeansoulin},
  title = {Design of a Domain Specific Language for modelling processes in landscapes},
  journal = {Ecological Modelling},
  year = {2009},
  volume = {220},
  pages = {3527 - 3535},
  number = {24},
  note = {<ce:title>Selected Papers on Spatially Explicit Landscape Modelling:
	Current practices and challenges</ce:title>},
  abstract = {The modelling of processes that occur in landscapes is often confronted
	to issues related to the representation of space and the difficulty
	of properly handling time and multiple scales. In order to investigate
	these issues, a flexible modelling environment is required. We propose
	to develop such a tool based on a Domain Specific Language (DSL)
	that capitalises on the service-oriented architecture (SOA) paradigm.
	The modelling framework around the DSL is composed of a model building
	environment, a code generator and compiler, and a program execution
	platform. The DSL introduces five language elements (entity, service,
	relation, scenario and datafacer) that can be combined to offer a
	wide range of possibilities for modelling in space and time at different
	scales. When developing a model, model parts are either built using
	the DSL or taken from libraries of previously built ones, and adapted
	to the specific model. The practical usage of the DSL is illustrated
	first with the Lotka–Volterra model, and then with a landscape
	modelling experiment on the spread of a mosquito-borne disease in
	the Sahelian region of West Africa. An interesting characteristic
	of this approach is the possibility of adding new elements into an
	existing model, and replacing others with more appropriate ones,
	thus allowing potentially complex models to be built from simpler
	parts.},
  doi = {10.1016/j.ecolmodel.2009.06.018},
  issn = {0304-3800},
  keywords = {<span style='font-style: italic'>Ocelet</span>},
  url = {http://www.sciencedirect.com/science/article/pii/S030438000900413X}
}

@ARTICLE{Demaille2009101,
  author = {Akim Demaille and Renaud Durlin and Nicolas Pierron and Benoît Sigoure},
  title = {Semantics Driven Disambiguation: A Comparison of Different Approaches},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2009},
  volume = {238},
  pages = {101 - 116},
  number = {5},
  note = {<ce:title>Proceedings of the 8th Workshop on Language Descriptions,
	Tools and Applications (LDTA 2008)</ce:title>},
  abstract = {Context-sensitive languages such as or can be parsed using a context-free
	but ambiguous grammar, which requires another stage, disambiguation,
	in order to select the single parse tree that complies with the language's
	semantical rules. Naturally, large and complex languages induce large
	and complex disambiguation stages. If, in addition, the parser should
	be extensible, for instance to enable the embedding of domain specific
	languages, the disambiguation techniques should feature traditional
	software-engineering qualities: modularity, extensibility, scalability
	and expressiveness. We evaluate three approaches to write disambiguation
	filters for SDF grammars: algebraic equations with ASF, rewrite-rules
	with programmable traversals for Stratego, and attribute grammars
	with TAG (TransformersAttribute Grammar), our system. To this end
	we introduce Phenix, a highly ambiguous language. Its “standard”
	grammar exhibits ambiguities inspired by those found in the and standard
	grammars. To evaluate modularity, the grammar is layered: it starts
	with a small core language, and several layers add new features,
	new production rules, and new ambiguities.},
  doi = {10.1016/j.entcs.2009.09.043},
  issn = {1571-0661},
  keywords = {<span style='FONT-VARIANT: small-caps'>Transformers</span>},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066109003971}
}

@ARTICLE{Diomidis200191,
  author = {Diomidis and Spinellis},
  title = {Notable design patterns for domain-specific languages},
  journal = {Journal of Systems and Software},
  year = {2001},
  volume = {56},
  pages = {91 - 99},
  number = {1},
  abstract = {The realisation of domain-specific languages (dsls) differs in fundamental
	ways from that of traditional programming languages. We describe
	eight recurring patterns that we have identified as being used for
	dsl design and implementation. Existing languages can be extended,
	restricted, partially used, or become hosts for dsls. Simple dsls
	can be implemented by lexical processing. In addition, dsls can be
	used to create front-ends to existing systems or to express complicated
	data structures. Finally, dsls can be combined using process pipelines.
	The patterns described form a pattern language that can be used as
	a building block for a systematic view of the software development
	process involving dsls.},
  doi = {10.1016/S0164-1212(00)00089-3},
  issn = {0164-1212},
  keywords = {Design patterns},
  url = {http://www.sciencedirect.com/science/article/pii/S0164121200000893}
}

@ARTICLE{Dobrzański1996873,
  author = {L.A Dobrzański and J. Madejski and W. Malina and W. Sitek},
  title = {The prototype of an expert system for the selection of high-speed
	steels for cutting tools},
  journal = {Journal of Materials Processing Technology},
  year = {1996},
  volume = {56},
  pages = {873 - 881},
  number = {1-4},
  note = {<ce:title>International Conference on Advances in Material and Processing
	Technologies</ce:title>},
  abstract = {Experience gained by a team of researchers developing a database system
	designed as an aid for engineers and students who work out the tools
	made from the high-speed steels shows that this sort of system needs
	a flexible yet powerful user interface. To solve this task a front-end
	expert system module is being designed. Present design parameters
	assume that the intelligent add-on shall consist of some 500÷1000
	rules enabling the user to compare the characteristics properties
	of various high-speed steels in respect to given applications, more,
	the technology generating module shall include the what-if option.
	To make all these new features as user friendly as possible it has
	been decided that the work should start with development of the linguistic
	task-oriented processor to enable the user to contact with the system
	in a subset of the natural language. To this end a prototype of a
	user interface was written in PROLOG and is presently being tested.
	In order to set the limits to the above mentioned task the dictionary
	was developed including all the necessary nouns, verbs, adjectives,
	numerals and sentence structures that are allowed to appear in the
	interface domain specific language.},
  doi = {10.1016/0924-0136(96)85119-3},
  issn = {0924-0136},
  url = {http://www.sciencedirect.com/science/article/pii/0924013696851193}
}

@ARTICLE{Dodero2010332,
  author = {Juan Manuel Dodero and Álvaro Martínez del Val and Jorge Torres},
  title = {An extensible approach to visually editing adaptive learning activities
	and designs based on services},
  journal = {Journal of Visual Languages \&amp; Computing},
  year = {2010},
  volume = {21},
  pages = {332 - 346},
  number = {6},
  note = {<ce:title>Special Issue on Visual Instructional Design Languages</ce:title>},
  abstract = {Learning management systems (LMS) provide an operational environment
	in which an online course can be created and later executed. Inter-operation
	between creators and their authoring facilities, and the LMS execution
	engine are based on defining standards and specifications, such as
	the IMS Learning Design (LD). Because an LMS better serves as a course
	player than as a course creator, a large number of approaches and
	environments for standards-compliant course authoring have been developed.
	These approaches and environments propose a number of issues that
	deal with how adaptations are edited and how to define the connection
	of learning activities with external learning applications and services.
	These questions have raised concern, mostly because of the excessive
	commitment of the creators’ methods and tools used with an educational
	modeling language, as well as the isolation of the language used
	to describe the course from the host LMS. This work describes an
	abstract, extendible language used to specify the learning design
	of a course, which can be transformed into any LD language as required
	by the execution environment. The language is used from a generative
	authoring environment that offers the possibility of editing web
	services as an additional resource to assess learning activities.},
  doi = {10.1016/j.jvlc.2010.08.007},
  issn = {1045-926X},
  keywords = {Domain-specific languages},
  url = {http://www.sciencedirect.com/science/article/pii/S1045926X10000480}
}

@ARTICLE{Ed200913,
  author = {Ed and Harcourt},
  title = {Policies of System Level Pipeline Modeling},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2009},
  volume = {238},
  pages = {13 - 23},
  number = {2},
  note = {<ce:title>Proceedings of the First Workshop on Generative Technologies
	(WGT) 2008.</ce:title>},
  abstract = {Pipelining is a well understood and often used implementation technique
	for increasing the performance of a hardware system. We develop several
	SystemC/C++ modeling techniques that allow us to quickly model, simulate,
	and evaluate pipelines. We employ a small domain specific language
	(DSL) based on resource usage patterns that automates the drudgery
	of boilerplate code needed to configure connectivity in simulation
	models. The DSL is embedded directly in the host modeling language
	SystemC/C++. Additionally we develop several techniques for parameterizing
	a pipeline's behavior based on policies of function, communication,
	and timing (performance modeling).},
  doi = {10.1016/j.entcs.2009.05.003},
  issn = {1571-0661},
  keywords = {pipeline},
  url = {http://www.sciencedirect.com/science/article/pii/S157106610900125X}
}

@ARTICLE{Fabry2008165,
  author = {Johan Fabry and Éric Tanter and Theo D’Hondt},
  title = {KALA: Kernel aspect language for advanced transactions},
  journal = {Science of Computer Programming},
  year = {2008},
  volume = {71},
  pages = {165 - 180},
  number = {3},
  abstract = {Transaction management is a known crosscutting concern. Previous research
	has been conducted to express this concern as an aspect. However,
	such work has used general-purpose aspect languages which lack a
	formal foundation, and most importantly are unable to express advanced
	models of transaction management. In this paper, we propose a domain-specific
	aspect language for advanced transaction management, called KALA,
	that overcomes these limitations. First, KALA is based on a recognized
	formalism for the domain of advanced transaction management, called
	ACTA. Second, as a consequence of being based on the ACTA formalism,
	KALA covers a wide variety of models for transaction management.
	Finally, being a domain-specific aspect language, KALA allows programmers
	to express their needs at a higher level of abstraction than what
	is achieved with general-purpose aspect languages. This paper reports
	on the design of KALA and its implementation over Java, based on
	the Reflex AOP kernel for domain-specific aspect languages.},
  doi = {10.1016/j.scico.2007.10.004},
  issn = {0167-6423},
  keywords = {Advanced transaction management},
  url = {http://www.sciencedirect.com/science/article/pii/S0167642307001992}
}

@ARTICLE{Fall20011,
  author = {Andrew Fall and Joseph Fall},
  title = {A domain-specific language for models of landscape dynamics},
  journal = {Ecological Modelling},
  year = {2001},
  volume = {141},
  pages = {1 - 18},
  number = {1-3},
  abstract = {Gaining insight into the dynamic nature of landscapes often involves
	the use of simulation models to explore potential changes over long
	time frames and extensive spatial areas. However, bridging the gap
	between conceptual models of landscape dynamics and their simulation
	on a computer can lead to many pitfalls. If implemented using a general-purpose
	programming language, the underlying model becomes hidden in the
	details of the computer code, making it difficult to compare the
	conceptual and implemented models, and to modify the model. Alternatively,
	previously built models may contain hidden assumptions and have limited
	adaptability. Domain-specific languages have been developed in a
	number of areas to facilitate the construction of models at a level
	closer to the conceptual model, thereby making model implementation
	more accessible to domain experts. Such tools to support modelling
	in the domain of landscape ecology can achieve a balance between
	the flexibility of programming and the structure and ease of using
	pre-built models. One of the goals of SELES (Spatially Explicit Landscape
	Event Simulator) has been to create a language for modelling landscape
	dynamics that provides ecologists and planners with an appropriate
	tool to address some of the problems that arise in model development.
	Our high-level, structured language separates the specification of
	model behaviour from the mechanics of its implementation, freeing
	landscape modellers from programming and allowing them to focus on
	the underlying model. This language is declarative and thus permits
	a clear representation of the conceptual model, which the SELES engine
	converts into a computer simulation of landscape change. Our structured
	framework guides the development of a broad class of spatio-temporal
	landscape models by aiding model prototyping, modification, verification,
	comparison, and re-use.},
  doi = {10.1016/S0304-3800(01)00334-9},
  issn = {0304-3800},
  keywords = {Cellular automata},
  url = {http://www.sciencedirect.com/science/article/pii/S0304380001003349}
}

@ARTICLE{Fontoura2000239,
  author = {Marcus Fontoura and Sérgio Crespo and Carlos José Lucena and Paulo
	S.C Alencar and Donald D Cowan},
  title = {Using viewpoints to derive object-oriented frameworks: a case study
	in the web-based education domain},
  journal = {Journal of Systems and Software},
  year = {2000},
  volume = {54},
  pages = {239 - 257},
  number = {3},
  abstract = {This paper is an experience report that illustrates the applicability
	of a viewpoint-based design method for the Web-based education (WBE)
	domain. The method is a new approach for domain analysis that generates
	an object-oriented framework from a set of concrete applications.
	These applications are defined as viewpoints, since they provide
	different perspectives of the framework domain. Various existent
	WBE environments have been used as viewpoints in our case study.
	The design method has been successfully applied for these viewpoints
	generating the ALADIN framework. The analysed WBE environments are
	presented through object-oriented diagrams. The implementation and
	use of ALADIN is discussed to validate the results of the case study.},
  doi = {10.1016/S0164-1212(00)00054-6},
  issn = {0164-1212},
  keywords = {Viewpoints},
  url = {http://www.sciencedirect.com/science/article/pii/S0164121200000546}
}

@ARTICLE{Gaucherel2006233,
  author = {Cédric. Gaucherel and Nathalie Giboire and Valérie Viaud and Thomas
	Houet and Jacques Baudry and Françoise Burel},
  title = {A domain-specific language for patchy landscape modelling: The Brittany
	agricultural mosaic as a case study},
  journal = {Ecological Modelling},
  year = {2006},
  volume = {194},
  pages = {233 - 243},
  number = {1-3},
  note = {<ce:title>Special Issue on the Fourth European Conference on Ecological
	Modelling</ce:title> <ce:subtitle>Selected Papers from the Fourth
	European Conference on Ecological Modelling, September 27 - October
	1, 2004, Bled, Slovenia</ce:subtitle> <xocs:full-name>Fourth European
	Conference on Ecological Modelling</xocs:full-name>},
  abstract = {Recent developments in landscape ecology have emphasised the functional
	role of heterogeneity of mosaics such as a “patchworks” of agricultural
	fields. Explicit (process) landscape models are important tools to
	study this functional role, although very few model are dedicated
	to patchy landscape dynamics. We construct such a model (L1) manipulating
	patches (polygons) instead of pixels by a combination of pure attributive
	or geometrical modifications. In addition, our aim was to build a
	perennial and dynamic software platform. Hence, the L1 platform contributes
	to the design of distinct scenarios, as well as distinct models of
	(forested, agricultural, suburban…) landscapes. It is designed
	around a kernel modelling, a generic patchy landscape that could
	be progressively driven to follow natural (climatic, ecological…)
	forces and human (sociological, economical…) decisions. We present,
	here, the first model of the L1 platform with an application on a
	Brittany agricultural landscape and four simplified scenarios. We
	focused on human decisions driving the patchy mosaic and demonstrated
	the importance of taking into account the different decision levels,
	from the main characteristics of the European Common Agricultural
	Policies (CAP) to the farmer individual land use allocation. Results
	are a set of rules that reproduces Brittany landscape evolutions
	with increasing complexity driving-decisions, and therefore, with
	increasing realism degree. They illustrate part of the L1 platform
	flexibility.},
  doi = {10.1016/j.ecolmodel.2005.10.026},
  issn = {0304-3800},
  keywords = {Categorical landscape},
  url = {http://www.sciencedirect.com/science/article/pii/S0304380005005211}
}

@ARTICLE{LabraGayo2001110,
  author = {J.E. Labra Gayo and M.C. Luengo Díez and J.M. Cueva Lovelle and
	A. Cernuda del Río},
  title = {LPS: A Language Prototyping System Using Modular Monadic Semantics},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2001},
  volume = {44},
  pages = {110 - 131},
  number = {2},
  note = {<ce:title>LDTA'01, First Workshop on Language Descriptions, Tools
	and Applications (a Satellite Event of ETAPS 2001)</ce:title>},
  abstract = {This paper describes LPS, a Language Prototyping System that facilitates
	the modular development of interpreters from semantic building blocks.
	The system is based on the integration of ideas from Modular Monadic
	Semantics and Generic Programming.
	
	To define a new programming language, the abstract syntax is described
	as the fixpoint of non-recursive pattern functors. For each functor
	an algebra is defined whose carrier is the computational monad obtained
	from the application of several monad transformers to a base monad.
	The interpreter is automatically generated by a catamorphism or,
	in some special cases, a monadic catamorphism.
	
	The system has been implemented as a domain-specific language embedded
	in Haskell and we have also implemented an interactive framework
	for language testing.},
  doi = {10.1016/S1571-0661(04)80923-X},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S157106610480923X}
}

@ARTICLE{Gil2010573,
  author = {Joseph (Yossi) Gil and Keren Lenz},
  title = {Simple and safe SQL queries with C++ templates},
  journal = {Science of Computer Programming},
  year = {2010},
  volume = {75},
  pages = {573 - 595},
  number = {7},
  note = {<ce:title>Generative Programming and Component Engineering (GPCE
	2007)</ce:title>},
  abstract = {Most large software applications rely on an external relational database
	for storing and managing persistent data. Typically, such applications
	interact with the database by first constructing strings that represent
	SQL statements, and then submitting these for execution by the database
	engine. The fact that these statements are only checked for correctness
	at runtime is a source for many potential defects, including type
	and syntax errors and vulnerability to injection attacks.
	
	The AraRat system presented here offers a method for dealing with
	these difficulties by coercing the host C++ compiler to do the necessary
	checks of the generated strings. A library of templates and preprocessor
	directives is used to embed in C++ a little language representing
	an augmented relational algebra formalism. Type checking of this
	embedded language, carried out by our template library, assures,
	at compile-time, the correctness and safety of the generated SQL
	strings. All SQL statements constructed by AraRat are guaranteed
	to be syntactically correct, and type safe with respect to the database
	schema. Moreover, AraRat statically ensures that the generated statements
	are immune to all injection attacks.
	
	The standard techniques of “expression templates” and “compile-time
	symbolic derivation” for compile-time representation of symbolic
	structures, are enhanced in our system. We demonstrate the support
	of a type system and a symbol table lookup of the symbolic structure.
	A key observation of this work is that type equivalence of instantiated
	nominally typed generics in C++ (as well as other languages, e.g.,
	Java) is structural rather than nominal. This makes it possible to
	embed the structural type system, characteristic to persistent data
	management, in the nominal type system of C++.
	
	For some of its advanced features, AraRat relies on two small extensions
	to the standard C++ language: the typeof pseudo operator and the
	__COUNTER__ preprocessor macro.},
  doi = {10.1016/j.scico.2010.01.004},
  issn = {0167-6423},
  keywords = {C++},
  url = {http://www.sciencedirect.com/science/article/pii/S0167642310000158}
}

@ARTICLE{Ginot200223,
  author = {Vincent Ginot and Christophe Le Page and Sami Souissi},
  title = {A multi-agents architecture to enhance end-user individual-based
	modelling},
  journal = {Ecological Modelling},
  year = {2002},
  volume = {157},
  pages = {23 - 41},
  number = {1},
  abstract = {The increasing importance of individual-based modelling (IBM) in population
	dynamics has led to the greater availability of tools designed to
	facilitate their creation and use. Yet, these tools are either too
	general, requiring the extensive knowledge of a computer language,
	or conversely restricted to very specific applications. Hence, they
	are of little help to non-computer expert ecologists. In order to
	build IBM's without hard coding them nor restricting their scope
	too much, we suggest a component programming, assuming that each
	elementary task that forms the behaviour of an individual often follows
	the same path: an individual must locate and select information in
	order for it to be processed, then he must update his state, the
	state of other individuals, or the state of the rest of the ‘world’.
	This sequence is well suited to translation into elementary computerised
	components, that we call primitives. Conversely, task building will
	involve stringing out well-chosen primitives and setting their parameter
	values or mathematical formulae. In order to restrict the number
	of primitives and to simplify their use, ‘information’ must be
	carried through well defined structures. We suggest the use of the
	multi-agents system paradigm (MAS) which originates from the distributed
	artificial intelligence and defines agents as autonomous objects
	that perceive and react to their environment. If one assumes that
	a model can be described entirely with the help of agents, then primitives
	only handle agents, agent state or history. This greatly simplifies
	their conception and enhances their flexibility. Indeed, only 25
	primitives, split into six groups (locate, select, translate, compute,
	end, and workflow control) proved to be sufficient to build complex
	IBM's or cellular automata drawn from literature. Furthermore, such
	a primitive-based multi-agents architecture is very flexible and
	facilitates all the steps of the modelling process, in particular
	the simulation engine (agents call and synchronisation), the results
	analysis, and the simulation experiments. Component programming may
	also facilitate the design of a domain specific language in which
	these models could be written and exported to other simulation platforms.},
  doi = {10.1016/S0304-3800(02)00211-9},
  issn = {0304-3800},
  keywords = {Individual-based models (IBM)},
  url = {http://www.sciencedirect.com/science/article/pii/S0304380002002119}
}

@ARTICLE{Glück2011347,
  author = {Robert Glück and Eelco Visser},
  title = {Special Issue on Generative Programming and Component Engineering
	(Selected Papers from GPCE 2004/2005)},
  journal = {Science of Computer Programming},
  year = {2011},
  volume = {76},
  pages = {347 - 348},
  number = {5},
  note = {<ce:title>Special Issue on Generative Programming and Component Engineering
	(Selected Papers from GPCE 2004/2005)</ce:title>},
  doi = {10.1016/j.scico.2011.02.001},
  issn = {0167-6423},
  url = {http://www.sciencedirect.com/science/article/pii/S0167642311000220}
}

@ARTICLE{Gokhale2007359,
  author = {Aniruddha Gokhale and Dimple Kaul and Arundhati Kogekar and Jeff
	Gray and Swapna Gokhale},
  title = {POSAML: A visual modeling language for middleware provisioning},
  journal = {Journal of Visual Languages \&amp; Computing},
  year = {2007},
  volume = {18},
  pages = {359 - 377},
  number = {4},
  note = {<ce:title>Visual Interactions in Software Artifacts</ce:title>},
  abstract = {Next generation distributed applications are often hosted on heterogeneous
	platforms including different kinds of middleware. Due to the applications’
	growing functional complexity and their multiple quality of service
	(QoS) requirements, system developers are increasingly facing a substantial
	number of middleware provisioning challenges, which include configuring,
	optimizing and validating the middleware platforms for QoS properties.
	Traditional techniques for middleware provisioning tend to use non-intuitive,
	low-level and technology-specific approaches, which are tedious,
	error prone, and non-reusable across different technologies. Quite
	often the middleware provisioning activities are carried out by different
	actors without much interaction among them, which results in an iterative
	trial-and-error process to provisioning. Higher level abstractions,
	particularly those that use visual models, are effective in addressing
	these challenges. This paper describes the design of a visual modeling
	language called POSAML (pattern-oriented software architecture modeling
	language) and associated tools that provide an intuitive, higher
	level and unified framework for provisioning middleware platforms.
	POSAML provides visual modeling capabilities for middleware-independent
	configurations and optimizations while enabling automated middleware-specific
	validation of system QoS properties.},
  doi = {10.1016/j.jvlc.2007.07.003},
  issn = {1045-926X},
  keywords = {Model-driven engineering},
  url = {http://www.sciencedirect.com/science/article/pii/S1045926X07000419}
}

@ARTICLE{Griss1995213,
  author = {Martin L. Griss and Kevin D. Wentzel},
  title = {Hybrid domain-specific kits},
  journal = {Journal of Systems and Software},
  year = {1995},
  volume = {30},
  pages = {213 - 230},
  number = {3},
  note = {<ce:title>Software Reuse</ce:title>},
  abstract = {As part of Hewlett-Packard Laboratories research into systematic,
	domain-specific reuse, we are exploring the notion of domain-specific
	kits. Kits are comprised of compatible, domain-specific components,
	frameworks, and languages, supported by a variety of well-integrated
	technologies and tools, such as domain-specific languages, builders,
	generators, and domain-tailored environments. We are particularly
	interested in hybrid kits, which make visible the combination of
	both generative and compositional reuse. We have prototyped several
	sample kits and have designed a framework for analyzing and comparing
	kits.},
  doi = {10.1016/0164-1212(94)00135-A},
  issn = {0164-1212},
  url = {http://www.sciencedirect.com/science/article/pii/016412129400135A}
}

@ARTICLE{Gulwani2001191,
  author = {S Gulwani and A Tarachandani and D Gupta and D Sanghi and L.P Barreto
	and G Muller and C Consel and Compose group},
  title = {WebCaL — a domain specific language for web caching},
  journal = {Computer Communications},
  year = {2001},
  volume = {24},
  pages = {191 - 201},
  number = {2},
  abstract = {Web caching aims to improve the performance of the Internet in three
	ways — by improving client latency, alleviating network traffic
	and reducing server load. A web cache is basically a limited store
	of information which helps in presenting a faster web-access environment
	to the clients. The performance of a cache depends on proper management
	of this information and effective inter-cache communication. The
	existing web caches have simple and hard-coded policies which are
	not best suited for all environments. They offer limited flexibility
	just in the form of changing some simple parameters such as cache
	size, peer caches, etc. This drawback motivates the need for a framework
	for building new web caches tailored to specific environments. In
	this paper, we describe a domain specific language based on an event-action
	model using which new local web cache policies and inter-cache protocols
	can be easily specified. This should make it possible to write a
	new policy or protocol quickly, evaluate its performance and test
	it thoroughly using the complete program-execute-debug cycle.},
  doi = {10.1016/S0140-3664(00)00314-5},
  issn = {0140-3664},
  keywords = {WebCal},
  url = {http://www.sciencedirect.com/science/article/pii/S0140366400003145}
}

@ARTICLE{Günther2011,
  author = {Sebastian Günther and Sagar Sunkle},
  title = {rbFeatures: Feature-oriented programming with Ruby},
  journal = {Science of Computer Programming},
  year = {2011},
  pages = { - },
  number = {0},
  abstract = {Features are pieces of core functionality of a program that is relevant
	to particular stakeholders. Features pose dependencies and constraints
	among each other. These dependencies and constraints describe the
	possible number of variants of the program: A valid feature configuration
	generates a specific variant with unique behavior. Feature-Oriented
	Programming is used to implement features as program units. This
	paper introduces rbFeatures, a feature-oriented programming language
	implemented on top of the dynamic programming language Ruby. With
	rbFeatures, programmers use software product lines, variants, and
	features as first-class entities. This allows several runtime reflection
	and modification capabilities, including the extension of the product
	line with new features and the provision of multiple variants. The
	paper gives a broad overview to the implementation and application
	of rbFeatures. We explain how features as first-class entities are
	designed and implemented, and discuss how the semantics of features
	are carefully added to Ruby programs. We show two case studies: The
	expression product line, a common example in feature-oriented programming,
	and a web application.},
  doi = {10.1016/j.scico.2010.12.007},
  issn = {0167-6423},
  keywords = {Feature-oriented programming},
  url = {http://www.sciencedirect.com/science/article/pii/S0167642311000025}
}

@ARTICLE{StuartH1992181,
  author = {Stuart H. and Rubin},
  title = {Case-based learning: A new paradigm for automated knowledge acquisition},
  journal = {ISA Transactions},
  year = {1992},
  volume = {31},
  pages = {181 - 209},
  number = {2},
  abstract = {Case-Based Learning (CBL) is a new paradigm for solving problems by
	generalizing and transforming solutions of similar previously encountered
	ones. Cases serve as actual problem-solution instances in CBL. Here,
	cases differ from those in the Case-Based Reasoning (CBR) paradigms
	in that they are immutable.
	
	Expertn systems define a network of intercting domain-specific subsystems
	for bootstrapping CBL methods. The primitive subsystems are called
	next-generation expert systems. All subsystems constrain random generalization
	spaces. Generalizations can be augmented with a CBL method called
	Random Seeded Crystal Learning (RSCL).
	
	RSCL methods generate a population of similar hypotheses using a tranformational
	paradigm. The cases serve to delimit this space while simultaneously
	refining the process for generating the hypotheses. RSCL methods
	provide for the shifting of the knowledge engineer's focus from the
	design of rules to the design and bootstrapping of domain-specific
	languages for their capture.
	
	A prototype CBL system called LS has been realized at NOSC. LS efficiently
	induces novel rules that are open under deduction. Results indicate
	that more knowledge can be generated than is supplied. LS is implemented
	on a DAP-610 platform in approximately 10, 000 lines of FORTRAN-Plus.
	Its utility—especially as applied to intelligent manufacturing
	systems—is expected to mirror advances in parallel hardware.},
  doi = {10.1016/0019-0578(92)90038-K},
  issn = {0019-0578},
  url = {http://www.sciencedirect.com/science/article/pii/001905789290038K}
}

@ARTICLE{Hamey200837,
  author = {Leonard G.C. Hamey and Shirley N. Goldrei},
  title = {Implementing a Domain-Specific Language Using Stratego/XT: An Experience
	Paper},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2008},
  volume = {203},
  pages = {37 - 51},
  number = {2},
  note = {<ce:title>Proceedings of the Seventh Workshop on Language Descriptions,
	Tools, and Applications (LDTA 2007)</ce:title>},
  abstract = {We describe the experience of implementing a Domain-Specific Language
	using transformation to a General Purpose Language. The domain of
	application is image processing and low-level computer vision. The
	transformation is accomplished using the Stratego/XT language transformation
	toolset. The implementation presented here is contrasted with the
	original implementation carried out many years ago using standard
	compiler implementation tools of the day. We highlight some of the
	unexpected advantages afforded to us, as language designers and implementers,
	by the source-to-source transformation technique. We also present
	some of the practical challenges faced in the implementation and
	show how these issues were addressed.},
  doi = {10.1016/j.entcs.2008.03.043},
  issn = {1571-0661},
  keywords = {Domain-specific language},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066108001485}
}

@ARTICLE{Hanus200335,
  author = {Michael Hanus and Klaus Höppner and Frank Huch},
  title = {Towards Translating Embedded Curry to C},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2003},
  volume = {86},
  pages = {35 - 51},
  number = {3},
  note = {<ce:title>WFLP 2003, 12th International Workshop on Functional and
	Constraint Logic Programming (in connection with RDP'03, Federated
	Conference on Rewriting, Deduction and Programming)</ce:title>},
  abstract = {This paper deals with a framework to program autonomous robots in
	the declarative multi-paradigm language Curry. Our goal is to apply
	a high-level declarative programming language for the programming
	of embedded systems. For this purpose, we use a specialization of
	Curry called Embedded Curry. We show the basic ideas of our framework
	and an implementation that translates Embedded Curry programs into
	C.},
  doi = {10.1016/S1571-0661(04)80692-3},
  issn = {1571-0661},
  keywords = {functional logic programming},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066104806923}
}

@ARTICLE{Harald20113,
  author = {Harald and Störrle},
  title = {VMQL: A visual language for ad-hoc model querying},
  journal = {Journal of Visual Languages \&amp; Computing},
  year = {2011},
  volume = {22},
  pages = {3 - 29},
  number = {1},
  note = {<ce:title>Special Issue on Visual Languages and Logic</ce:title>},
  abstract = {In large scale model based development, analysis level models are
	more like knowledge bases than engineering artifacts. Their effectiveness
	depends, to a large degree, on the ability of domain experts to retrieve
	information from them ad-hoc. For large scale models, however, existing
	query facilities are inadequate.
	
	The visual model query language (VMQL) is a novel approach that uses
	the respective modeling language of the source model as the query
	language, too. The semantics of VMQL is defined formally based on
	graphs, so that query execution can be defined as graph matching.
	VMQL has been applied to several visual modeling languages, implemented,
	and validated in small case studies, and several controlled experiments.},
  doi = {10.1016/j.jvlc.2010.11.004},
  issn = {1045-926X},
  keywords = {Model querying},
  url = {http://www.sciencedirect.com/science/article/pii/S1045926X1000073X}
}

@ARTICLE{Harmanci20101053,
  author = {Derin Harmanci and Vincent Gramoli and Pascal Felber and Christof
	Fetzer},
  title = {Extensible transactional memory testbed},
  journal = {Journal of Parallel and Distributed Computing},
  year = {2010},
  volume = {70},
  pages = {1053 - 1067},
  number = {10},
  note = {<ce:title>Transactional Memory</ce:title>},
  abstract = {Transactional Memory (TM) is a promising abstraction as it hides all
	synchronization complexities from the programmers of concurrent applications.
	More particularly, the TM paradigm operated a complexity shift from
	the application programming to the TM programming. Therefore, expert
	programmers have now started to look for the ideal TM that will bring,
	once-for-all, performance to all concurrent applications. Researchers
	have recently identified numerous issues TMs may suffer from. Surprisingly,
	no TMs have ever been tested in these scenarios. In this paper, we
	present the first to date TM testbed. We propose a framework, TMunit,
	that provides a domain specific language to write rapidly TM workloads
	so that our test-suite is easily extensible. Our reproducible semantic
	tests indicate through reproducible counter-examples that existing
	TMs do not satisfy recent consistency criteria. Our performance tests
	identify workloads where well-known TMs perform differently. Finally,
	additional tests indicate some workloads preventing contention managers
	from progressing.},
  doi = {10.1016/j.jpdc.2010.02.008},
  issn = {0743-7315},
  keywords = {Transactional memory},
  url = {http://www.sciencedirect.com/science/article/pii/S0743731510000328}
}

@ARTICLE{Hemel2011150,
  author = {Zef Hemel and Danny M. Groenewegen and Lennart C.L. Kats and Eelco
	Visser},
  title = {Static consistency checking of web applications with WebDSL},
  journal = {Journal of Symbolic Computation},
  year = {2011},
  volume = {46},
  pages = {150 - 182},
  number = {2},
  note = {<ce:title>Automated Specification and Verification of Web Systems</ce:title>},
  abstract = {Modern web application development frameworks provide web application
	developers with high-level abstractions to improve their productivity.
	However, their support for static verification of applications is
	limited. Inconsistencies in an application are often not detected
	statically, but appear as errors at run-time. The reports about these
	errors are often obscure and hard to trace back to the source of
	the inconsistency. A major part of this inadequate consistency checking
	can be traced back to the lack of linguistic integration of these
	frameworks. Parts of an application are defined with separate domain-specific
	languages, which are not checked for consistency with the rest of
	the application. Examples include regular expressions, query languages
	and XML-based languages for definition of user interfaces. We give
	an overview and analysis of typical problems arising in development
	with frameworks for web application development, with Ruby on Rails,
	Lift and Seam as representatives.
	
	To remedy these problems, in this paper, we argue that domain-specific
	languages should be designed from the ground up with static verification
	and cross-aspect consistency checking in mind, providing linguistic
	integration of domain-specific sub-languages. We show how this approach
	is applied in the design of WebDSL, a domain-specific language for
	web applications, by examining how its compiler detects inconsistencies
	not caught by web frameworks, providing accurate and clear error
	messages. Furthermore, we show how this consistency analysis can
	be expressed with a declarative rule-based approach using the Stratego
	transformation language.},
  doi = {10.1016/j.jsc.2010.08.006},
  issn = {0747-7171},
  keywords = {Domain-specific language},
  url = {http://www.sciencedirect.com/science/article/pii/S0747717110001367}
}

@ARTICLE{Heradio200925,
  author = {R. Heradio and J.A. Cerrada and J.C. Lopez and J.R. Coz},
  title = {Code Generation with the Exemplar Flexibilization Language},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2009},
  volume = {238},
  pages = {25 - 34},
  number = {2},
  note = {<ce:title>Proceedings of the First Workshop on Generative Technologies
	(WGT) 2008.</ce:title>},
  abstract = {Code Generation is an increasing popular technique for implementing
	Software Product Lines that produces code from abstract specifications
	written in Domain Specific Languages (DSLs). This paper proposes
	to take advantage of the similitude among the products in a domain
	to generate them by analogy. That is, instead of synthesizing the
	final code from scratch or transforming the DSL specifications, the
	final products are obtained by adapting a previously developed domain
	product. The paper also discusses the capabilities and limitations
	of several currently available tools and languages to implement this
	kind of generators and introduce a new language to overcome the limitations.},
  doi = {10.1016/j.entcs.2009.05.004},
  issn = {1571-0661},
  keywords = {Code Generation},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066109001261}
}

@ARTICLE{Herrmann200647,
  author = {Christoph A. Herrmann and Tobias Langhammer},
  title = {Combining partial evaluation and staged interpretation in the implementation
	of domain-specific languages},
  journal = {Science of Computer Programming},
  year = {2006},
  volume = {62},
  pages = {47 - 65},
  number = {1},
  note = {<ce:title>Special Issue on the First MetaOCaml Workshop 2004</ce:title>},
  abstract = {We propose a combination of partial evaluation and staged interpretation
	with MetaOCaml for rapid prototyping of domain-specific languages.
	Interpretation is an easy way to implement such languages. MetaOCaml
	can eliminate the overhead of interpretation at run-time, if the
	interpreter is written in a staged form, i.e., takes the source program
	separate from the input data in a first stage. Partial evaluation
	of the source program with values known at compile time can further
	improve the target code performance. Additional aggressive optimizations
	are possible due to the absence of general recursion. Algebraic simplifications
	can even achieve binding-time improvements during the online partial
	evaluation. Our approach both saves the application programmer completely
	from binding-time considerations and exploits staged interpretation
	with MetaOCaml for target code generation.
	
	The example domain presented in this paper is image processing, in
	which the domain-specific language permits the specification of convolution
	matrices, summations, case distinctions and non-local pixel accesses.
	All expressions known at compile time are simplified and all remaining
	expressions are turned into MetaOCaml code parts, which are combined
	to form the compiled application program.
	
	The example specifications deal with filtering by convolution and
	iterations in a series of images for wave effects and temperature
	distribution.
	
	The experimental results show significant speed-ups if online partial
	evaluation with algebraic simplifications is used for the elimination
	of interpretation overhead and optimization of code expressions.},
  doi = {10.1016/j.scico.2006.02.002},
  issn = {0167-6423},
  keywords = {Binding-time improvement},
  url = {http://www.sciencedirect.com/science/article/pii/S0167642306000736}
}

@ARTICLE{Herrmannsdörfer2010121,
  author = {Markus Herrmannsdörfer and Benjamin Hummel},
  title = {Library Concepts for Model Reuse},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2010},
  volume = {253},
  pages = {121 - 134},
  number = {7},
  note = {<ce:title>Proceedings of the Ninth Workshop on Language Descriptions
	Tools and Applications (LDTA 2009)</ce:title>},
  abstract = {Reuse and the composition of libraries of partial system descriptions
	is a fundamental and well-understood practice in software engineering,
	as long as we talk about source code. For models and modeling languages,
	the concepts of reuse often are limited to copy &amp; paste, especially
	when it comes to domain-specific modeling languages (DSLs). This
	paper attempts to give an overview of techniques for including support
	for reuse and library concepts both in the meta-model and the modeling
	tool, and presents a novel generative approach for this task. The
	technical consequences for each of the approaches presented are discussed
	and compared to each other.},
  doi = {10.1016/j.entcs.2010.08.036},
  issn = {1571-0661},
  keywords = {reuse},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066110001155}
}

@ARTICLE{Huang2011376,
  author = {Shan Shan Huang and David Zook and Yannis Smaragdakis},
  title = {Statically safe program generation with SafeGen},
  journal = {Science of Computer Programming},
  year = {2011},
  volume = {76},
  pages = {376 - 391},
  number = {5},
  note = {<ce:title>Special Issue on Generative Programming and Component Engineering
	(Selected Papers from GPCE 2004/2005)</ce:title>},
  abstract = {SafeGen is a meta-programming language for writing statically safe
	generators of Java programs. If a program generator written in SafeGen
	passes the checks of the SafeGen compiler, then the generator will
	only generate well-formed Java programs, for any generator input.
	In other words, statically checking the generator guarantees the
	correctness of any generated program, with respect to static checks
	commonly performed by a conventional compiler (including type safety,
	existence of a superclass, etc.). To achieve this guarantee, SafeGen
	supports only language primitives for reflection over an existing
	well-formed Java program, primitives for creating program fragments,
	and a restricted set of constructs for iteration, conditional actions,
	and name generation. SafeGen’s static checking algorithm is a combination
	of traditional type checking for Java, and a series of calls to a
	theorem prover to check the validity of first-order logical sentences,
	constructed to represent well-formedness properties of the generated
	program under all inputs. The approach has worked quite well in our
	tests, providing proofs for correct generators or pointing out interesting
	bugs.},
  doi = {10.1016/j.scico.2008.09.007},
  issn = {0167-6423},
  keywords = {Meta-programming},
  url = {http://www.sciencedirect.com/science/article/pii/S0167642308001111}
}

@ARTICLE{CánovasIzquierdo2011,
  author = {Javier Luis Cánovas Izquierdo and Frédéric Jouault and Jordi Cabot
	and Jesús García Molina},
  title = {API2MoL: Automating the building of bridges between APIs and Model-Driven
	Engineering},
  journal = {Information and Software Technology},
  year = {2011},
  pages = { - },
  number = {0},
  abstract = {Context A software artefact typically makes its functionality available
	through a specialized Application Programming Interface (API) describing
	the set of services offered to client applications. In fact, building
	any software system usually involves managing a plethora of APIs,
	which complicates the development process. In Model-Driven Engineering
	(MDE), where models are the key elements of any software engineering
	activity, this API management should take place at the model level.
	Therefore, tools that facilitate the integration of APIs and MDE
	are clearly needed. Objective Our goal is to automate the implementation
	of API–MDE bridges for supporting both the creation of models from
	API objects and the generation of such API objects from models. In
	this sense, this paper presents the API2MoL approach, which provides
	a declarative rule-based language to easily write mapping definitions
	to link API specifications and the metamodel that represents them.
	These definitions are then executed to convert API objects into model
	elements or vice versa. The approach also allows both the metamodel
	and the mapping to be automatically obtained from the API specification
	(bootstrap process). Method After implementing the API2MoL engine,
	its correctness was validated using several APIs. Since APIs are
	normally large, we then developed a tool to implement the bootstrap
	process, which was also validated. Results We provide a toolkit (language
	and bootstrap tool) for the creation of bridges between APIs and
	MDE. The current implementation focuses on Java APIs, although its
	adaptation to other statically typed object-oriented languages is
	straightforward. The correctness, expressiveness and completeness
	of the approach have been validated with the Swing, SWT and JTwitter
	APIs. Conclusion API2MoL frees developers from having to manually
	implement the tasks of obtaining models from API objects and generating
	such objects from models. This helps to manage API models in MDE-based
	solutions.},
  doi = {10.1016/j.infsof.2011.09.006},
  issn = {0950-5849},
  keywords = {Application Programming Interface},
  url = {http://www.sciencedirect.com/science/article/pii/S0950584911001984}
}

@ARTICLE{Jan200954,
  author = {Jan and Jürjens},
  title = {A domain-specific language for cryptographic protocols based on streams},
  journal = {Journal of Logic and Algebraic Programming},
  year = {2009},
  volume = {78},
  pages = {54 - 73},
  number = {2},
  abstract = {Developing security-critical systems is difficult and there are many
	well-known examples of security weaknesses exploited in practice.
	Thus a sound methodology supporting secure systems development is
	urgently needed. In particular, an important missing link in the
	construction of secure systems is finding a practical way to create
	reliably secure crypto protocol implementations. We present an approach
	that aims to address this need by making use of a domain-specific
	language for crypto protocol implementations. One can use this language
	to construct a compact and precise yet executable representation
	of a cryptographic protocol. This high-level program can be verified
	against the security goals using automated theorem provers for first
	order logic. One can then use it to provide assurance for legacy
	implementations of crypto protocols by generating test-cases.},
  doi = {10.1016/j.jlap.2008.08.006},
  issn = {1567-8326},
  keywords = {Cryptographic protocols},
  url = {http://www.sciencedirect.com/science/article/pii/S1567832608000787}
}

@ARTICLE{Jouvelot2011113,
  author = {Pierre Jouvelot and Yann Orlarey},
  title = {Dependent vector types for data structuring in multirate Faust},
  journal = {Computer Languages, Systems \&amp; Structures},
  year = {2011},
  volume = {37},
  pages = {113 - 131},
  number = {3},
  abstract = {Faust is a functional programming language dedicated to the specification
	of executable monorate synchronous musical applications. To extend
	Faust capabilities to important domains such as FFT-based spectral
	processing, we introduce here a multirate extension of the core Faust
	language. The novel idea is to link rate changes to data structure
	manipulation operations. Creating a vector-valued output signal divides
	the rate of input signals by the vector size, while serializing vectors
	multiplies rates accordingly. As duals to vectors, we also introduce
	record-like data structures, which are used to gather data but do
	not change signal rates. This interplay between data structures and
	rates is made possible in the language static semantics by the introduction
	of dependent types. We present a typing semantics, a denotational
	semantics and correctness theorems that show that this data structuring/multirate
	extension preserves the language synchronous characteristics. This
	new design is under implementation in the Faust compiler.},
  doi = {10.1016/j.cl.2011.03.001},
  issn = {1477-8424},
  keywords = {Domain specific languages},
  url = {http://www.sciencedirect.com/science/article/pii/S1477842411000029}
}

@ARTICLE{FisterJr2011151,
  author = {Iztok Fister Jr. and Iztok Fister and Marjan Mernik and Janez Brest},
  title = {Design and implementation of domain-specific language easytime},
  journal = {Computer Languages, Systems \&amp; Structures},
  year = {2011},
  volume = {37},
  pages = {151 - 167},
  number = {4},
  abstract = {Measuring time in mass sporting competitions is, typically, performed
	with a timing system that consists of a measuring technology and
	a computer system. The first is dedicated to tracking events that
	are triggered by competitors and registered by measuring devices
	(primarily based on RFID technology). The latter enables the processing
	of these events. In this paper, the processing of events is performed
	by an agent that is controlled by the domain-specific language, EasyTime.
	EasyTime improves the flexibility of the timing system because it
	supports the measuring of time in various sporting competitions,
	their quick adaptation to the demands of new sporting competitions
	and a reduction in the number of measuring devices. Essentially,
	we are focused on the development of a domain specific language.
	In practice, we made two case studies of using EasyTime by measuring
	time in two different sporting competitions. The use of EasyTime
	showed that it can be useful for sports clubs and competition organizers
	by aiding in the results of smaller sporting competitions, while
	in larger sporting competitions it could simplify the configuration
	of the timing system.},
  doi = {10.1016/j.cl.2011.04.001},
  issn = {1477-8424},
  keywords = {Domain-specific language},
  url = {http://www.sciencedirect.com/science/article/pii/S1477842411000145}
}

@ARTICLE{Juergens200655,
  author = {Elmar Juergens and Markus Pizka},
  title = {The Language Evolver Lever — Tool Demonstration —},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2006},
  volume = {164},
  pages = {55 - 60},
  number = {2},
  note = {<ce:title>Proceedings of the Sixth Workshop on Language Descriptions,
	Tools, and Applications (LDTA 2006)</ce:title> <xocs:full-name>Sixth
	Workshop on Language Descriptions, Tools, and Applications</xocs:full-name>},
  abstract = {Since many domains are constantly evolving, the associated domain
	specific languages (DSL) inevitably have to evolve too, to retain
	their value. But the evolution of a DSL can be very expensive, since
	existing words of the language (i.e. programs) and tools have to
	be adapted according to the changes of the DSL itself. In such cases,
	these costs seriously limit the adoption of DSLs.
	
	This paper presents Lever, a tool for the evolutionary development
	of DSLs. Lever aims at making evolutionary changes to a DSL much
	cheaper by automating the adaptation of the DSL parser as well as
	existing words and providing additional support for the correct adaptation
	of existing tools (e.g. program generators). This way, Lever simplifies
	DSL maintenance and paves the ground for bottom-up DSL development.},
  doi = {10.1016/j.entcs.2006.10.004},
  issn = {1571-0661},
  keywords = {domain specific languages},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066106004816}
}

@ARTICLE{K20021007,
  author = {K. and Åhlander},
  title = {Einstein summation for multidimensional arrays},
  journal = {Computers \&amp; Mathematics with Applications},
  year = {2002},
  volume = {44},
  pages = {1007 - 1017},
  number = {8-9},
  abstract = {One of the most common data structures, at least in scientific computing,
	is the multidimensional array. Some numerical algorithms may conveniently
	be expressed as a generalized matrix multiplication, which computes
	a multidimensional array from two other multidimensional arrays.
	By adopting index notation with the Einstein summation convention,
	an elegant tool for expressing generalized matrix multiplications
	is obtained. Index notation is the succinct and compact notation
	primarily used in tensor calculus.
	
	In this paper, we develop computer support for index notation as a
	domain specific language. Grammar and semantics are proposed, yielding
	an unambiguous interpretation algorithm. An object-oriented implementation
	of a C++ library that supports index notation is described.
	
	A key advantage with computer support of index notation is that the
	notational gap between a mathematical index notation algorithm and
	its implementation in a computer language is avoided. This facilitates
	program construction as well as program understanding. Program examples
	that demonstrate the close resemblance between code and the original
	mathematical formulation are presented.},
  doi = {10.1016/S0898-1221(02)00210-9},
  issn = {0898-1221},
  keywords = {Index notation},
  url = {http://www.sciencedirect.com/science/article/pii/S0898122102002109}
}

@ARTICLE{Kandare2010419,
  author = {Gregor Kandare and Stanislav Strmčnik and Giovanni Godena},
  title = {Domain specific model-based development of software for programmable
	logic controllers},
  journal = {Computers in Industry},
  year = {2010},
  volume = {61},
  pages = {419 - 431},
  number = {5},
  abstract = {Procedural process control is responsible for coordination of control
	units that perform basic control in a typical industrial control
	system. Basic control, in turn, performs actions necessary for maintaining
	a desired state of process variables and equipment. Software in the
	domain of procedural process control consists of modules responsible
	for management of startup and shutdown sequences, exception handling
	and module communication. In this work we present the domain specific
	modeling language (DSL) ProcGraph together with its corresponding
	code generation tool that was designed for the development of software
	in the domain of procedural process control systems. The advantage
	of using a domain specific language is that not only the programmers,
	but also domain experts are able to understand and modify the code.
	The DSL code is self-documenting, as it is expressed in the idiom
	of the problem domain. In the article we present a formal description
	of the ProcGraph language. Furthermore, we describe how the formal
	model is used in the implementation of the automatic IEC 1131-3 code
	generator.},
  doi = {10.1016/j.compind.2009.10.001},
  issn = {0166-3615},
  keywords = {Procedural process control},
  url = {http://www.sciencedirect.com/science/article/pii/S0166361509001924}
}

@ARTICLE{Karaila200529,
  author = {Mika Karaila and Tarja Systä},
  title = {On the Role of Metadata in Visual Language Reuse and Reverse Engineering
	– An Industrial Case},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2005},
  volume = {137},
  pages = {29 - 41},
  number = {3},
  note = {<ce:title>Proceedings of the 2nd International Workshop on Metamodels,
	Schemas, and Grammars for Reverse Engineering (ateM 2004)</ce:title>
	<xocs:full-name>Metamodels, Schemas, and Grammars for Reverse Engineering
	2004</xocs:full-name>},
  abstract = {Collecting metadata on a family of programs is useful not only for
	generating statistical data on the programs but also for future re-engineering
	and reuse purposes. In this paper we discuss an industrial case where
	a project library is used to store visual programs and a database
	to store the metadata on these programs. The visual language in question
	is a domain-specific language, Function Block Language (FBL) that
	is used in Metso Automation for writing automation control programs.
	For reuse, program analysis and re-engineering activities and various
	data and program analysis methods are applied to study the FBL programs.
	Metadata stored in a database is used to provide advanced program
	analysis support; from the large amount of programs, the metadata
	allows focusing the analysis to certain kinds of programs. In this
	paper, we discuss the role and usage of the metadata in program analysis
	techniques applied to FBL programs.},
  doi = {10.1016/j.entcs.2005.07.003},
  issn = {1571-0661},
  keywords = {Visual languages},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066105051030}
}

@ARTICLE{Kastens20065,
  author = {Uwe Kastens and Carsten Schmidt},
  title = {Visual Patterns Associated to Abstract Trees},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2006},
  volume = {148},
  pages = {5 - 18},
  number = {1},
  note = {<ce:title>Proceedings of the School of SegraVis Research Training
	Network on Foundations of Visual Modelling Techniques (FoVMT 2004)</ce:title>
	<xocs:full-name>Foundations of Visual Modelling Techniques 2004</xocs:full-name>},
  abstract = {Visual languages have an important role in modeling systems, specification
	of software, and in specific application domains. By using visual
	properties like spatial placement or line connections complex structures
	can be presented, so that humans can understand them quickly. Visual
	languages can be based on domain-specific metaphors, so that domain
	specialists can use their conventional way of description and abstraction.
	
	For working with a visual language, a specialized graphical frontend
	is needed. In contrast to textual languages, general purpose editors
	are insufficient for visual languages, because each visual language
	has its particular graphical requirements. The frontend should provide
	methods to aid efficient drawing and restructuring of visual expressions.
	Often, language-specific structure editors are used as frontends
	for visual languages. The visual program is stored in a language-dependent
	data structure. The user interacts with one or more visual representations.
	Edit operations are directly applied to the underlying structure
	and after a change the graphical representation is recomputed.
	
	The implementation of visual languages requires a wide range of conceptual
	and technical knowledge from issues of user interface design and
	graphical implementation to aspects of analysis and transformation
	for languages in general. We present a powerful toolset that incorporates
	such knowledge [C. Schmidt and U. Kastens. Implementation of visual
	languages using pattern-based specifications. Software - Practice
	and Experience, 35(2):121–131, Nov. 2003]. It generates editors
	from high level specifications: A language is specified by identifying
	certain patterns in the language structure, selecting a visual representation
	from a set of precoined solutions, and associating the pattern to
	constructs of the abstract grammar. A complete visual structure editor
	is generated from such a specification. It represents visual programs
	by attributed abstract trees. Therefore, further phases of processing
	visual programs can be generated by state-of-the-art tools for compiler
	construction. Even challenging visual languages can be implemented
	with reasonable small effort and with rather limited technical knowledge.
	The approach is suitable for a large variety of visual language styles.},
  doi = {10.1016/j.entcs.2005.12.010},
  issn = {1571-0661},
  keywords = {Visual languages},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066106000405}
}

@ARTICLE{Kats2010149,
  author = {Lennart C.L. Kats and Karl T. Kalleberg and Eelco Visser},
  title = {Domain-Specific Languages for Composable Editor Plugins},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2010},
  volume = {253},
  pages = {149 - 163},
  number = {7},
  note = {<ce:title>Proceedings of the Ninth Workshop on Language Descriptions
	Tools and Applications (LDTA 2009)</ce:title>},
  abstract = {Modern IDEs increase developer productivity by incorporating many
	different kinds of editor services. These can be purely syntactic,
	such as syntax highlighting, code folding, and an outline for navigation;
	or they can be based on the language semantics, such as in-line type
	error reporting and resolving identifier declarations. Building all
	these services from scratch requires both the extensive knowledge
	of the sometimes complicated and highly interdependent APIs and extension
	mechanisms of an IDE framework, and an in-depth understanding of
	the structure and semantics of the targeted language. This paper
	describes Spoofax/IMP, a meta-tooling suite that provides high-level
	domain-specific languages for describing editor services, relieving
	editor developers from much of the framework-specific programming.
	Editor services are defined as composable modules of rules coupled
	to a modular SDF grammar. The composability provided by the SGLR
	parser and the declaratively defined services allows embedded languages
	and language extensions to be easily formulated as additional rules
	extending an existing language definition. The service definitions
	are used to generate Eclipse editor plugins. We discuss two examples:
	an editor plugin for WebDSL, a domain-specific language for web applications,
	and the embedding of WebDSL in Stratego, used for expressing the
	(static) semantic rules of WebDSL.},
  doi = {10.1016/j.entcs.2010.08.038},
  issn = {1571-0661},
  keywords = {Domain specific language},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066110001179}
}

@ARTICLE{Klint20083,
  author = {P. Klint and A.T. Kooiker and J.J. Vinju},
  title = {Language Parametric Module Management for IDEs},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2008},
  volume = {203},
  pages = {3 - 19},
  number = {2},
  note = {<ce:title>Proceedings of the Seventh Workshop on Language Descriptions,
	Tools, and Applications (LDTA 2007)</ce:title>},
  abstract = {An integrated development environment (IDE) monitors all the changes
	that a user makes to source code modules and responds accordingly
	by flagging errors, by reparsing, by rechecking, or by recompiling
	modules and by adjusting visualizations or other information derived
	from a module. A module manager is the central component of the IDE
	that is responsible for this behavior. Although the overall functionality
	of a module manager in a given IDE is fixed, its actual behavior
	strongly depends on the programming languages it has to support.
	What is a module? How do modules depend on each other? What is the
	effect of a change to a module?
	
	We propose a concise design for a language parametric module manager:
	a module manager that is parameterized with the module behavior of
	a specific language. We describe the design of our module manager
	and discuss some of its properties. We also report on the application
	of the module manager in the construction of IDEs for the specification
	language Asf+Sdf as well as for Java.
	
	Our overall goal is the rapid development (generation) of IDEs for
	programming languages and domain specific languages. The module manager
	presented here represents a next step in the creation of such generic
	language workbenches.},
  doi = {10.1016/j.entcs.2008.03.041},
  issn = {1571-0661},
  keywords = {module management},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066108001461}
}

@ARTICLE{Kloos2010145,
  author = {Johannes Kloos and Robert Eschbach},
  title = {A Systematic Approach to Construct Compositional Behaviour Models
	for Network-structured Safety-critical Systems},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2010},
  volume = {263},
  pages = {145 - 160},
  number = {0},
  note = {<ce:title>Proceedings of the 6th International Workshop on Formal
	Aspects of Component Software (FACS 2009)</ce:title>},
  abstract = {This paper considers the problem of model-based testing of a class
	of safety-critical systems. These systems are built up from components
	that are connected a network-like structure. The number of possible
	structures is usually large. In particular, we consider the following
	issue: For many of these systems, each instance needs its own set
	of models for testing. On the other hand, the instances that should
	be tested will have to be chosen so that the reliability statements
	are generally applicable. Thus, they must be chosen by a domain expert.
	The approach in this paper addresses both of these points. The structure
	of the instance of system under test is described using a domain-specific
	language, so that a domain expert can easily describe a system instance
	for testing. At the same time, the components and composition operators
	are formalized. Using a structure description written in the DSL,
	corresponding test models can be automatically generated, allowing
	for automated testing by the domain expert. We show some evidence
	about the feasibility of our approach and about the effort required
	for modelling an example, supporting our belief that our approach
	improves both on the efficiency and the expressivity of current compositional
	test model construction techniques.},
  doi = {10.1016/j.entcs.2010.05.009},
  issn = {1571-0661},
  keywords = {Model-based testing},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066110000496}
}

@ARTICLE{Kos2011,
  author = {Tomaž Kos and Tomaž Kosar and Marjan Mernik},
  title = {Development of data acquisition systems by using a domain-specific
	modeling language},
  journal = {Computers in Industry},
  year = {2011},
  pages = { - },
  number = {0},
  abstract = {Data acquisition is the process of capturing and measuring physical
	data and then converting the results into a digital form that is
	further manipulated by a computer program. Within the industry, data
	acquisition systems (measurement systems) are used in a wide variety
	of fields, including product quality testing. Usually measuring systems
	are complicated devices, however newer data acquisition systems tend
	to be easier to use. As such, they open the door for the development
	of customized software, which can be easily manipulated, not only
	by programmers but also by domain experts, enabling them to understand
	and modify programs. Raising the level of abstraction, particularly
	with those programs that use visual models, can be an effective aid
	for domain experts, who are then able to model their programs on
	their own. This paper describes the design and use of a domain-specific
	modeling language called the Sequencer, integrated with the measuring
	equipment DEWESoft, which enables domain experts to model their own
	data acquisitions. Specifically, in this paper the Sequencer is exposed
	to: domain concepts identification, the construction of modeling
	notation, a connection with execution framework, and the end-users’
	point of view on the modeling tool. The use of the Sequencer will
	be presented on car brake tests. For this purpose, the Sequencer
	has already been successfully applied in the automotive industry.},
  doi = {10.1016/j.compind.2011.09.004},
  issn = {0166-3615},
  keywords = {Domain-specific modeling languages},
  url = {http://www.sciencedirect.com/science/article/pii/S0166361511001059}
}

@ARTICLE{Kosar2008390,
  author = {Tomaž Kosar and Pablo E. Martı´nez López and Pablo A. Barrientos
	and Marjan Mernik},
  title = {A preliminary study on various implementation approaches of domain-specific
	language},
  journal = {Information and Software Technology},
  year = {2008},
  volume = {50},
  pages = {390 - 405},
  number = {5},
  abstract = {Various implementation approaches for developing a domain-specific
	language are available in literature. There are certain common beliefs
	about the advantages/disadvantages of these approaches. However,
	it is hard to be objective and speak in favor of a particular one,
	since these implementation approaches are normally compared over
	diverse application domains.
	
	The purpose of this paper is to provide empirical results from ten
	diverse implementation approaches for domain-specific languages,
	but conducted using the same representative language. Comparison
	shows that these discussed approaches differ in terms of the effort
	need to implement them, however, the effort needed by a programmer
	to implement a domain-specific language should not be the only factor
	taken into consideration. Another important factor is the effort
	needed by an end-user to rapidly write correct programs using the
	produced domain-specific language. Therefore, this paper also provides
	empirical results on end-user productivity, which is measured as
	the lines of code needed to express a domain-specific program, similarity
	to the original notation, and how error-reporting and debugging are
	supported in a given implementation.},
  doi = {10.1016/j.infsof.2007.04.002},
  issn = {0950-5849},
  keywords = {Domain-specific languages},
  url = {http://www.sciencedirect.com/science/article/pii/S0950584907000419}
}

@ARTICLE{JuliaL2010471,
  author = {Julia L. and Lawall},
  title = {Preface to special issue on Generative Programming and Component
	Engineering (GPCE 2007)},
  journal = {Science of Computer Programming},
  year = {2010},
  volume = {75},
  pages = {471 - 472},
  number = {7},
  note = {<ce:title>Generative Programming and Component Engineering (GPCE
	2007)</ce:title>},
  doi = {10.1016/j.scico.2010.01.005},
  issn = {0167-6423},
  url = {http://www.sciencedirect.com/science/article/pii/S016764231000016X}
}

@ARTICLE{Lerner20055,
  author = {Sorin Lerner and Todd Millstein and Craig Chambers},
  title = {Cobalt: A Language for Writing Provably-Sound Compiler Optimizations},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2005},
  volume = {132},
  pages = {5 - 17},
  number = {1},
  note = {<ce:title>Proceedings of the 3rd International Workshop on Compiler
	Optimization Meets Compiler Verification (COCV 2004)</ce:title> <xocs:full-name>Compiler
	Optimization Meets Compiler Verification 2004</xocs:full-name>},
  abstract = {We overview the current status and future directions of the Cobalt
	project. Cobalt is a domain-specific language for implementing compiler
	optimizations as guarded rewrite rules. Cobalt optimizations operate
	over a C-like intermediate representation including unstructured
	control flow, pointers to local variables and dynamically allocated
	memory, and recursive procedures. The design of Cobalt engenders
	a natural inductive strategy for proving the soundness of optimizations.
	This strategy is fully automated by requiring an automatic theorem
	prover to discharge a small set of simple proof obligations for each
	optimization. We have written a variety of forward and backward intraprocedural
	dataflow optimizations in Cobalt, including constant propagation
	and folding, branch folding, full and partial redundancy elimination,
	full and partial dead assignment elimination, and simple forms of
	points-to analysis. The implementation of our soundness-checking
	strategy employs the Simplify automatic theorem prover, and we have
	used this implementation to automatically prove the above optimizations
	correct. An execution engine for Cobalt optimizations is implemented
	as part of the Whirlwind compiler infrastructure.},
  doi = {10.1016/j.entcs.2005.03.022},
  issn = {1571-0661},
  keywords = {Compiler optimization},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066105050024}
}

@ARTICLE{Limbourg2008815,
  author = {Philipp Limbourg and Hans-Dieter Kochs},
  title = {Multi-objective optimization of generalized reliability design problems
	using feature models—A concept for early design stages},
  journal = {Reliability Engineering \&amp; System Safety},
  year = {2008},
  volume = {93},
  pages = {815 - 828},
  number = {6},
  abstract = {Reliability optimization problems such as the redundancy allocation
	problem (RAP) have been of considerable interest in the past. However,
	due to the restrictions of the design space formulation, they may
	not be applicable in all practical design problems. A method with
	high modelling freedom for rapid design screening is desirable, especially
	in early design stages. This work presents a novel approach to reliability
	optimization. Feature modelling, a specification method originating
	from software engineering, is applied for the fast specification
	and enumeration of complex design spaces. It is shown how feature
	models can not only describe arbitrary RAPs but also much more complex
	design problems. The design screening is accomplished by a multi-objective
	evolutionary algorithm for probabilistic objectives. Comparing averages
	or medians may hide the true characteristics of this distributions.
	Therefore the algorithm uses solely the probability of a system dominating
	another to achieve the Pareto optimal set. We illustrate the approach
	by specifying a RAP and a more complex design space and screening
	them with the evolutionary algorithm.},
  doi = {10.1016/j.ress.2007.03.032},
  issn = {0951-8320},
  keywords = {Feature modelling},
  url = {http://www.sciencedirect.com/science/article/pii/S0951832007001299}
}

@ARTICLE{Linehan2011,
  author = {Eamonn Linehan and Siobhán Clarke},
  title = {An aspect-oriented, model-driven approach to functional hardware
	verification},
  journal = {Journal of Systems Architecture},
  year = {2011},
  pages = { - },
  number = {0},
  abstract = {The cost of correcting errors in the design of an embedded system’s
	hardware components can be higher than for its software components,
	making it important to test as early as possible. Testing hardware
	components before they are implemented involves verifying the design
	through either formal or more commonly, simulation-based functional
	verification. Performing functional verification of a hardware design
	requires software-based simulators and verification testbenches.
	However, the increasing complexity of embedded systems is contributing
	to testbenches that are progressively more difficult to understand,
	maintain, extend and reuse across projects. This paper presents an
	aspect-oriented domain-specific modelling language for the e hardware
	verification language that can be used as part of a model-based software
	engineering process. The modelling language is designed to produce
	well modularised models from which e code can be generated, thereby
	improving engineers ability to develop testbenches that can be more
	easily maintained, adapted and reused. We demonstrate the suitability
	of the modelling language through its application to a representative
	testbench from the automotive semiconductor industry.},
  doi = {10.1016/j.sysarc.2011.02.001},
  issn = {1383-7621},
  keywords = {Model-based software engineering},
  url = {http://www.sciencedirect.com/science/article/pii/S138376211100018X}
}

@ARTICLE{Lung2010672,
  author = {Chung-Horng Lung and Pragash Rajeswaran and Sathyanarayanan Sivadas
	and Theleepan Sivabalasingam},
  title = {Experience of building an architecture-based generator using GenVoca
	for distributed systems},
  journal = {Science of Computer Programming},
  year = {2010},
  volume = {75},
  pages = {672 - 688},
  number = {8},
  note = {<ce:title>Designing high quality system/software architectures</ce:title>},
  abstract = {Selecting the architecture that meets the requirements, both functional
	and non-functional, is a challenging task, especially at the early
	stage when more uncertainties exist. Architectural prototyping is
	a useful approach in supporting the evaluation of alternative architectures
	and balancing different architectural qualities. Generative programming
	has gained increasing attention, but it mostly deals with lower-level
	artifacts; hence, it usually supports lower degrees of software automation.
	This paper proposes an architecture-centric generative approach in
	facilitating architectural prototyping and evaluation. We also present
	our empirical experience in raising the level of abstraction to the
	architecture layer for distributed and concurrent systems using GenVoca.
	GenVoca is a generative programming approach that is used here to
	support the generation or instantiation of a particular architectural
	pattern in distributed computing based on user’s selection. As
	a result, it can support rapid architectural prototyping and evaluation
	of both functional and non-functional requirements and encourage
	greater degrees of software automation and reuse. Lessons learned
	from the empirical study are also reported and could be applied to
	other areas.},
  doi = {10.1016/j.scico.2009.05.003},
  issn = {0167-6423},
  keywords = {Generative programming},
  url = {http://www.sciencedirect.com/science/article/pii/S0167642309000896}
}

@ARTICLE{Maraninchi2003219,
  author = {Florence Maraninchi and Yann Rémond},
  title = {Mode-Automata: a new domain-specific construct for the development
	of safe critical systems},
  journal = {Science of Computer Programming},
  year = {2003},
  volume = {46},
  pages = {219 - 254},
  number = {3},
  note = {<ce:title>Special issue on Formal Methods for Industrial Critical
	Systems</ce:title>},
  abstract = {Over the past ten years, the family of synchronous languages (Special
	Section of the Proc. IEEE 79 (9) (1991)) has been very successful
	in offering domain-specific, formally defined languages and programming
	environments for safety-critical systems. Among them, Lustre is well-suited
	for the development of regulation systems, which are first designed
	by control engineers, and can then be programmed as block-diagrams.
	Automatic generation of C code provides the embedded software.
	
	The success of Lustre showed that it is a good idea to offer domain-specific
	languages and constructs to reduce the gap between the first design
	of a system (for instance a control law) and the program written
	for it. When the structure of the first design has to be encoded
	into the available constructs of a general-purpose programming language,
	the interesting information is likely to be lost somewhere on the
	way from the original design to the actual implementation. This may
	have consequences on the efficiency of the code produced, or even
	on the correctness of the design.
	
	Working with the systems Lustre is well-suited for, we observed that
	they are often specified informally using the notion of running modes.
	However, there seemed to exist no language in which the mode-structure
	of a complex system could be expressed directly. Following the approach
	of domain-specific languages, we proposed to extend Lustre with a
	new construct, called mode-automaton, devoted to the description
	of these running modes of regulation systems.
	
	In this paper, we define the language of mode-automata and its semantics,
	give some ideas on the compilation process, illustrate the approach
	with the example of the production cell, and comment on the benefits
	of the approach, in general.},
  doi = {10.1016/S0167-6423(02)00093-X},
  issn = {0167-6423},
  keywords = {Real-time systems},
  url = {http://www.sciencedirect.com/science/article/pii/S016764230200093X}
}

@ARTICLE{Markall20101815,
  author = {Graham R. Markall and David A. Ham and Paul H.J. Kelly},
  title = {Towards generating optimised finite element solvers for GPUs from
	high-level specifications},
  journal = {Procedia Computer Science},
  year = {2010},
  volume = {1},
  pages = {1815 - 1823},
  number = {1},
  note = {<ce:title>ICCS 2010</ce:title>},
  abstract = {We argue that producing maintainable high-performance implementations
	of finite element methods for multiple targets requires that they
	are written using a high-level domain-specific language. We make
	the case for using one such language, the Unified Form Language (UFL),
	by discussing how it allows the generation of high-performance code
	from maintainable sources. We support this case by showing that optimal
	implementations of a finite element solver written for a Graphics
	Processing Unit and a multicore CPU require the use of different
	algorithms and data formats that are embodied by the UFL representation.
	Finally we describe a prototype compiler that generates low-level
	code from high-level specifications, and outline how the high-level
	UFL representation can be lowered to facilitate optimisation using
	existing techniques prior to code generation.},
  doi = {10.1016/j.procs.2010.04.203},
  issn = {1877-0509},
  keywords = {Finite element method},
  url = {http://www.sciencedirect.com/science/article/pii/S1877050910002048}
}

@ARTICLE{MartínezOrtiz20091092,
  author = {Iván Martínez-Ortiz and José-Luis Sierra and Baltasar Fernández-Manjón
	and Alfredo Fernández-Valmayor},
  title = {Language engineering techniques for the development of e-learning
	applications},
  journal = {Journal of Network and Computer Applications},
  year = {2009},
  volume = {32},
  pages = {1092 - 1105},
  number = {5},
  note = {<ce:title>Next Generation Content Networks</ce:title>},
  abstract = {In this paper we propose the use of language engineering techniques
	to improve and systematize the development of e-learning applications.
	E-learning specifications usually rely on domain-specific languages
	that describe different aspects of such final e-learning applications.
	This fact makes it natural to adopt well-established language engineering
	principles during the construction of these applications. These principles
	promote the specification of the structure and the runtime behavior
	of the domain-specific languages as the central part of the development
	process. This specification can be used to drive different activities:
	rapid prototyping, provision of authoring notations and tools, automatic
	model checking of properties, importation/exportation from/to standards,
	and deployment of running applications. This language engineering
	concept also promotes active collaboration between instructors (the
	users of the languages) and developers (the designers and implementers)
	throughout the development process. In this paper we describe this
	language-driven approach to the construction of e-learning applications
	and we illustrate all its aspects using a learning flow sequencing
	language as a case study.},
  doi = {10.1016/j.jnca.2009.02.005},
  issn = {1084-8045},
  keywords = {E-learning applications},
  url = {http://www.sciencedirect.com/science/article/pii/S1084804509000459}
}

@ARTICLE{McIntosh2007224,
  author = {Anne C.S. McIntosh and Judith B. Cushing and Nalini M. Nadkarni and
	Lee Zeman},
  title = {Database design for ecologists: Composing core entities with observations},
  journal = {Ecological Informatics},
  year = {2007},
  volume = {2},
  pages = {224 - 236},
  number = {3},
  note = {<ce:title>Meta-information systems and ontologies. A Special Feature
	from the 5th International Conference on Ecological Informatics ISEI5,
	Santa Barbara, CA, Dec. 4–7, 2006</ce:title> <ce:subtitle>Novel
	Concepts of Ecological Data Management S.I.</ce:subtitle>},
  abstract = {The ecoinformatics community recognizes that ecological synthesis
	across studies, space, and time will require new informatics tools
	and infrastructure. Recent advances have been encouraging, but many
	problems still face ecologists who manage their own datasets, prepare
	data for archiving, and search data stores for synthetic research.
	In this paper, we describe how work by the Canopy Database Project
	(CDP) might enable use of database technology by field ecologists:
	increasing the quality of database design, improving data validation,
	and providing structural and semantic metadata — all of which might
	improve the quality of data archives and thereby help drive ecological
	synthesis.
	
	The CDP has experimented with conceptual components for database design,
	templates, to address information technology issues facing ecologists.
	Templates represent forest structures and observational measurements
	on these structures. Using our software, researchers select templates
	to represent their study’s data and can generate normalized relational
	databases. Information hidden in those databases is used by ancillary
	tools, including data intake forms and simple data validation, data
	visualization, and metadata export. The primary question we address
	in this paper is, which templates are the right templates.
	
	We argue for defining simple templates (with relatively few attributes)
	that describe the domain's major entities, and for coupling those
	with focused and flexible observation templates. We present a conceptual
	model for the observation data type, and show how we have implemented
	the model as an observation entity in the DataBank database designer
	and generator. We show how our visualization tool CanopyView exploits
	metadata made explicit by DataBank to help scientists with analysis
	and synthesis. We conclude by presenting future plans for tools to
	conduct statistical calculations common to forest ecology and to
	enhance data mining with DataBank databases.
	
	DataBank could be extended to another domain by replacing our forest–ecology-specific
	templates with those for the new domain. This work extends the basic
	computer science idea of abstract data types and user-defined types
	to ecology-specific database design tools for individual users, and
	applies to ecoinformatics the software engineering innovations of
	domain-specific languages, software patterns, components, refactoring,
	and end-user programming.},
  doi = {10.1016/j.ecoinf.2007.07.003},
  issn = {1574-9541},
  keywords = {Data visualization},
  url = {http://www.sciencedirect.com/science/article/pii/S1574954107000465}
}

@ARTICLE{Meyers20111223,
  author = {Bart Meyers and Hans Vangheluwe},
  title = {A framework for evolution of modelling languages},
  journal = {Science of Computer Programming},
  year = {2011},
  volume = {76},
  pages = {1223 - 1246},
  number = {12},
  note = {<ce:title>Special Issue on Software Evolution, Adaptability and Variability</ce:title>},
  abstract = {In model-driven engineering, evolution is inevitable over the course
	of the complete life cycle of complex software-intensive systems
	and more importantly of entire product families. Not only instance
	models, but also entire modelling languages are subject to change.
	This is in particular true for domain-specific languages, whose language
	constructs are tightly coupled to an application domain.
	
	The most popular approach to evolution in the modelling domain is
	a manual process, with tedious and error-prone migration of artefacts
	such as instance models as a result. This paper provides a taxonomy
	for evolution of modelling languages and discusses the different
	evolution scenarios for various kinds of modelling artefacts, such
	as instance models, meta-models, and transformation models. Subsequently,
	the consequences of evolution and the required remedial actions are
	decomposed into primitive scenarios such that all possible evolutions
	can be covered exhaustively. These primitives are then used in a
	high-level framework for the evolution of modelling languages.
	
	We suggest that our structured approach enables the design of (semi-)automatic
	modelling language evolution solutions.},
  doi = {10.1016/j.scico.2011.01.002},
  issn = {0167-6423},
  keywords = {Evolution},
  url = {http://www.sciencedirect.com/science/article/pii/S0167642311000141}
}

@ARTICLE{MorenoGer2009564,
  author = {Pablo Moreno-Ger and Rubén Fuentes-Fernández and José-Luis Sierra-Rodríguez
	and Baltasar Fernández-Manjón},
  title = {Model-checking for adventure videogames},
  journal = {Information and Software Technology},
  year = {2009},
  volume = {51},
  pages = {564 - 580},
  number = {3},
  abstract = {This paper describes a model-checking approach for adventure games
	focusing on 〈e-Adventure〉, a platform for the development of
	adaptive educational adventure videogames. In 〈e-Adventure〉,
	games are described using a domain-specific language oriented to
	game writers. By defining a translation from this language to suitable
	state-based models, it is possible to automatically extract a verification
	model for each 〈e-Adventure〉 game. In addition, temporal properties
	to be verified are described using an extensible assertion language,
	which can be tailored to each specific application scenario. When
	the framework determines that some of these properties do not hold,
	it generates an animation of a counterexample. This approach facilitates
	the collaboration of multidisciplinary teams of experts during the
	verification of the integrity of the game scripts, exchanging hours
	of manual verification for semi-automatic verification processes
	that also facilitate the diagnosis of the conditions that may potentially
	break the games.},
  doi = {10.1016/j.infsof.2008.08.003},
  issn = {0950-5849},
  keywords = {Adventure games},
  url = {http://www.sciencedirect.com/science/article/pii/S0950584908001134}
}

@ARTICLE{Nelson2001459,
  author = {M.A.V Nelson and P.S.C Alencar and D.D Cowan},
  title = {An approach to formal specification and verification of map-centered
	applications},
  journal = {Environmental Modelling \&amp; Software},
  year = {2001},
  volume = {16},
  pages = {459 - 465},
  number = {5},
  note = {<ce:title>Design principles for environmental information systems</ce:title>},
  abstract = {Users in the domain of map-centered applications who want to specify
	a new application rely only on informal languages such as English.
	There is also no standardized terminology, resulting in ambiguous
	specifications. This work proposes an approach to specify and verify
	map-centered applications. This domain has been studied under different
	perspectives but there is a lack of research from the software engineering
	viewpoint. We characterize the domain by presenting a classification
	of different space models that appear in the Geographical Information
	Systems (GIS) literature, as well as of some of the problems addressed
	by map-centered applications. The proposed solution includes a language
	with semantics based in a formalization of each space model. This
	helps in verifying properties over a specification written using
	the formal language.},
  doi = {10.1016/S1364-8152(01)00017-2},
  issn = {1364-8152},
  keywords = {Map-centered applications},
  url = {http://www.sciencedirect.com/science/article/pii/S1364815201000172}
}

@ARTICLE{Ouyang2007162,
  author = {Chun Ouyang and Eric Verbeek and Wil M.P. van der Aalst and Stephan
	Breutel and Marlon Dumas and Arthur H.M. ter Hofstede},
  title = {Formal semantics and analysis of control flow in WS-BPEL},
  journal = {Science of Computer Programming},
  year = {2007},
  volume = {67},
  pages = {162 - 198},
  number = {2-3},
  abstract = {Web service composition refers to the creation of new (Web) services
	by combining functionalities provided by existing ones. A number
	of domain-specific languages for service composition have been proposed,
	with consensus being formed around a process-oriented language known
	as WS-BPEL (or BPEL). The kernel of BPEL consists of simple communication
	primitives that may be combined using control-flow constructs expressing
	sequence, branching, parallelism, synchronization, etc. We present
	a comprehensive and rigorously defined mapping of BPEL constructs
	onto Petri net structures, and use this for the analysis of various
	dynamic properties related to unreachable activities, conflicting
	messages, garbage collection, conformance checking, and deadlocks
	and lifelocks in interaction processes. We use a mapping onto Petri
	nets because this allows us to use existing theoretical results and
	analysis tools. Unlike approaches based on finite state machines,
	we do not need to construct the state space, and can use structural
	analysis (e.g., transition invariants) instead. We have implemented
	a tool that translates BPEL processes into Petri nets and then applies
	Petri-net-based analysis techniques. This tool has been tested on
	different examples, and has been used to answer a variety of questions.},
  doi = {10.1016/j.scico.2007.03.002},
  issn = {0167-6423},
  keywords = {Business process modeling},
  url = {http://www.sciencedirect.com/science/article/pii/S0167642307000500}
}

@ARTICLE{Padioleau200747,
  author = {Yoann Padioleau and Julia L. Lawall and Gilles Muller},
  title = {SmPL: A Domain-Specific Language for Specifying Collateral Evolutions
	in Linux Device Drivers},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2007},
  volume = {166},
  pages = {47 - 62},
  number = {0},
  note = {<ce:title>Proceedings of the ERCIM Working Group on Software Evolution
	(2006)</ce:title>},
  abstract = {Collateral evolutions are a pervasive problem in large-scale software
	development. Such evolutions occur when an evolution that affects
	the interface of a generic library entails modifications, i.e., collateral
	evolutions, in all library clients. Performing these collateral evolutions
	requires identifying the affected files and modifying all of the
	code fragments in these files that in some way depend on the changed
	interface.
	
	We have studied the collateral evolution problem in the context of
	Linux device drivers. Currently, collateral evolutions in Linux are
	mostly done manually using a text editor, possibly with the help
	of tools such as grep. The large number of Linux drivers, however,
	implies that this approach is time-consuming and unreliable, leading
	to subtle errors when modifications are not done consistently.
	
	In this paper, we propose a transformation language, SmPL, to specify
	collateral evolutions. Because Linux programmers are accustomed to
	exchanging, reading, and manipulating program modifications in terms
	of patches, we build our language around the idea and syntax of a
	patch, extending patches to semantic patches.},
  doi = {10.1016/j.entcs.2006.07.022},
  issn = {1571-0661},
  keywords = {Linux},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066106005287}
}

@ARTICLE{Pardillo20102591,
  author = {Jesús Pardillo and Cristina Cachero},
  title = {Domain-specific language modelling with UML profiles by decoupling
	abstract and concrete syntaxes},
  journal = {Journal of Systems and Software},
  year = {2010},
  volume = {83},
  pages = {2591 - 2606},
  number = {12},
  note = {<ce:title>TAIC PART 2009 - Testing: Academic \&amp; Industrial Conference
	- Practice And Research Techniques</ce:title>},
  abstract = {UML profiling presents some acknowledged deficiencies, among which
	the lack of expressiveness of the profiled notations, together with
	the high coupling between abstract and concrete syntaxes outstand.
	These deficiencies may cause distress among UML-profile modellers,
	who are often forced to extend from unsuitable metaclasses for mere
	notational reasons, or even to model domain-specific languages from
	scratch just to avoid the UML-profiling limitations.
	
	In order to palliate this situation, this article presents an extension
	of the UML profile metamodel to support arbitrarily-complex notational
	extensions by decoupling the UML abstract and concrete syntax. Instead
	of defining yet another metamodel for UML-notational profiling, notational
	extensions are modelled with DI, i.e., the UML notation metamodel
	for diagram interchange, keeping in this way the extension within
	the standard. Profiled UML notations are rendered with DI by defining
	the graphical properties involved, the domain-specific constraints
	applied to DI, and the rendering routines associated. Decoupling
	abstract and concrete syntax in UML profiles increases the notation
	expressiveness while decreasing the abstract-syntax complexity.},
  doi = {10.1016/j.jss.2010.08.019},
  issn = {0164-1212},
  keywords = {UML},
  url = {http://www.sciencedirect.com/science/article/pii/S0164121210002153}
}

@ARTICLE{Parigot200297,
  author = {Didier Parigot and Carine Courbis and Pascal Degenne and Alexandre
	Fau and Claude Pasquier and Joël Fillon and Christophe Held and
	Isabelle Attali},
  title = {Aspect and XML-oriented Semantic Framework Generator: SmartTools},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2002},
  volume = {65},
  pages = {97 - 116},
  number = {3},
  note = {<ce:title>LDTA 2002, Second Workshop on Language Descriptions, Tools
	and Applications (Satellite Event of ETAPS 2002)</ce:title>},
  abstract = {SmartTools is a semantic framework generator, based on XML and object
	technologies. Thanks to a process of automatic generation from specifications,
	SmartTools makes it possible to quickly develop environments dedicated
	to domain-specific and programming languages. Some of these specifications
	(XML, DTD, Schemas, XSLT) are issued from the W3C which is an important
	source of varied emerging domain-specific languages. SmartTools uses
	object technologies such as visitor patterns and aspect-oriented
	programming. It provides code generation adapted to the usage of
	those technologies to support the development of semantic analyses.
	In this way, we obtain at minimal cost the design and implementation
	of a modular development platform which is open, interactive, uniform,
	and most important prone to evolution.},
  doi = {10.1016/S1571-0661(04)80429-8},
  issn = {1571-0661},
  keywords = {software generation},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066104804298}
}

@ARTICLE{Paul2008187,
  author = {Paul and Graunke},
  title = {Verified Safety and Information Flow of a Block Device},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2008},
  volume = {217},
  pages = {187 - 202},
  number = {0},
  note = {<ce:title>Proceedings of the 3rd International Workshop on Systems
	Software Veriﬁcation (SSV 2008)</ce:title>},
  abstract = {This work reports on the author's experience designing, implementing,
	and formally verifying a low-level piece of system software. The
	timing model and the adaptation of an existing information flow policy
	to a monadic framework are reasonably novel. Interactive compilation
	through equational rewriting worked well in practice. Finally, the
	project uncovered some potential areas for improving interactive
	theorem provers.},
  doi = {10.1016/j.entcs.2008.06.049},
  issn = {1571-0661},
  keywords = {multi-level security},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066108003927}
}

@ARTICLE{Peter2002233,
  author = {Peter and Thiemann},
  title = {Programmable Type Systems for Domain Specific Languages},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2002},
  volume = {76},
  pages = {233 - 251},
  number = {0},
  note = {<ce:title>WFLP 2002, 11th International Workshop on Functional and
	(Constraint) Logic Programming, Selected Papers</ce:title>},
  abstract = {A language with a programmable type system is vital for the construction
	of an embedded domain specific language (EDSL). Driven by the requirements
	posed by the implementation of an EDSL for server-side Web scripting,
	we examine two major of extensions to the type system of the host
	language, Haskell. We show that a component that ensures the generation
	of correct HTML documents can take good advantage of type-level functions,
	as implemented using functional logic overloading. We further show
	that a function that ensures the consistency of data submitted to
	a Web script with the data expected by the script is less awkward
	to use in the presence of lambda expressions in the type language.
	In both cases we assess the guarantees obtained by the use of the
	typing and explore alternative solutions.},
  doi = {10.1016/S1571-0661(04)80796-5},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066104807965}
}

@ARTICLE{Pop2005137,
  author = {Adrian Pop and Ilie Savga and Uwe Aßmann and Peter Fritzson},
  title = {Composition of XML Dialects: A ModelicaXML Case Study},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2005},
  volume = {114},
  pages = {137 - 152},
  number = {0},
  note = {<ce:title>Proceedings of the Software Composition Workshop (SC 2004)</ce:title>
	<xocs:full-name>Software Composition Workshop 2004</xocs:full-name>},
  abstract = {This paper investigates how software composition and transformation
	can be applied to domain specific languages used today in modeling
	and simulation of physical systems. More specifically, we address
	the composition and transformation of the Modelica language. The
	composition targets the ModelicaXML dialect which is the XML representation
	of the Modelica language. By extending the COMPOST concrete composition
	layer with a component model for Modelica, we provide composition
	and transformation of Modelica. The design of our COMPOST extension
	is presented togheter with examples of composition programs for Modelica.},
  doi = {10.1016/j.entcs.2004.02.071},
  issn = {1571-0661},
  keywords = {Composition of XML dialects},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066104052703}
}

@ARTICLE{Ralf20081,
  author = {Ralf and Lämmel},
  title = {Google’s MapReduce programming model — Revisited},
  journal = {Science of Computer Programming},
  year = {2008},
  volume = {70},
  pages = {1 - 30},
  number = {1},
  abstract = {Google’s MapReduce programming model serves for processing large
	data sets in a massively parallel manner. We deliver the first rigorous
	description of the model including its advancement as Google’s
	domain-specific language Sawzall. To this end, we reverse-engineer
	the seminal papers on MapReduce and Sawzall, and we capture our findings
	as an executable specification. We also identify and resolve some
	obscurities in the informal presentation given in the seminal papers.
	We use typed functional programming (specifically Haskell) as a tool
	for design recovery and executable specification. Our development
	comprises three components: (i) the basic program skeleton that underlies
	MapReduce computations; (ii) the opportunities for parallelism in
	executing MapReduce computations; (iii) the fundamental characteristics
	of Sawzall’s aggregators as an advancement of the MapReduce approach.
	Our development does not formalize the more implementational aspects
	of an actual, distributed execution of MapReduce computations.},
  doi = {10.1016/j.scico.2007.07.001},
  issn = {0167-6423},
  keywords = {Data processing},
  url = {http://www.sciencedirect.com/science/article/pii/S0167642307001281}
}

@INCOLLECTION{Rangarajan20111743,
  author = {Srinivas Rangarajan and Ted Kaminski and Eric Van Wyk and Aditya
	Bhan and Prodromos Daoutidis},
  title = {Network generation and analysis of complex biomass conversion systems},
  booktitle = {21st European Symposium on Computer Aided Process Engineering},
  publisher = {Elsevier},
  year = {2011},
  editor = {E.N. Pistikopoulos, M.C. Georgiadis and A.C. Kokossis},
  volume = {29},
  series = {Computer Aided Chemical Engineering},
  pages = {1743 - 1747},
  abstract = {Abstract A modular computational tool for automated generation and
	rule-based post-processing of reaction systems in biomass conversion
	is presented. Cheminformatics and graph theory algorithms are used
	to generate chemical transformations pertaining to heterogeneous
	and homogeneous chemistries in the automated rule-based network generator.
	A domain-specific language provides a user-friendly English-like
	chemistry specification interface to the network generator. A rule-based
	pathway analysis module enables the user to extract and query pathways
	from the reaction network. A demonstration of the features of this
	tool is presented using Fructose to 5-Hydroxymethylfurfural as a
	case study.},
  doi = {10.1016/B978-0-444-54298-4.50127-6},
  issn = {1570-7946},
  keywords = {Network generation},
  url = {http://www.sciencedirect.com/science/article/pii/B9780444542984501276}
}

@ARTICLE{Robinson2008481,
  author = {Jon Robinson and Ian Wakeman and Dan Chalmers},
  title = {Composing software services in the pervasive computing environment:
	Languages or APIs?},
  journal = {Pervasive and Mobile Computing},
  year = {2008},
  volume = {4},
  pages = {481 - 505},
  number = {4},
  abstract = {The pervasive computing environment will be composed of heterogeneous
	services. In this work, we have explored how a domain specific language
	for service composition can be implemented to capture the common
	design patterns for service composition, yet still retain a comparable
	performance to other systems written in mainstream languages such
	as Java. In particular, we have proposed the use of the method delegation
	design pattern, the resolution of service bindings through the use
	of dynamically adjustable characteristics and the late binding of
	services as key features in simplifying the service composition task.
	These are realised through the Scooby language, and the approach
	is compared to the use of APIs to define adaptable services.},
  doi = {10.1016/j.pmcj.2008.01.001},
  issn = {1574-1192},
  keywords = {Pervasive computing},
  url = {http://www.sciencedirect.com/science/article/pii/S1574119208000035}
}

@ARTICLE{Rok20112259,
  author = {Rok and Žitko},
  title = {SNEG – Mathematica package for symbolic calculations with second-quantization-operator
	expressions},
  journal = {Computer Physics Communications},
  year = {2011},
  volume = {182},
  pages = {2259 - 2264},
  number = {10},
  abstract = {In many-particle problems involving interacting fermions or bosons,
	the most natural language for expressing the Hamiltonian, the observables,
	and the basis states is the language of the second-quantization operators.
	It thus appears advantageous to write numerical computer codes which
	allow the user to define the problem and the quantities of interest
	directly in terms of operator strings, rather than in some low-level
	programming language. Here I describe a Mathematica package which
	provides a flexible framework for performing the required translations
	between several different representations of operator expressions:
	condensed notation using pure ASCII character strings, traditional
	notation (“pretty printing”), internal Mathematica representation
	using nested lists (used for automatic symbolic manipulations), and
	various higher-level (“macro”) expressions. The package consists
	of a collection of transformation rules that define the algebra of
	operators and a comprehensive library of utility functions. While
	the emphasis is given on the problems from solid-state and atomic
	physics, the package can be easily adapted to any given problem involving
	non-commuting operators. It can be used for educational and demonstration
	purposes, but also for direct calculations of problems of moderate
	size. Program summary Program title: SNEG
	
	Catalogue identifier: AEJL_vl_0
	
	Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEJL_vl_0.html
	
	Program obtainable from: CPC Program Library, Queenʼs University,
	Belfast, N. Ireland
	
	Licensing provisions: GNU General Public License
	
	No. of lines in distributed program, including test data, etc.: 319 808
	
	No. of bytes in distributed program, including test data, etc.: 1 081 247
	
	Distribution format: tar.gz
	
	Programming language: Mathematica
	
	Computer: Any computer which runs Mathematica
	
	Operating system: Any OS which runs Mathematica
	
	RAM: Problem dependent
	
	Classification: 2.9, 5, 6.2
	
	Nature of problem: Manipulation of expressions involving second-quantization
	operators and other non-commuting objects. Calculation of commutators,
	anticommutators, expectation values. Generation of matrix representations
	of the Hamiltonians expressed in the second-quantization language.
	
	Solution method: Automatic reordering of operator strings in some
	well specified canonical order; (anti)commutation rules are used
	where needed. States may be represented in occupation-number representation.
	Dirac bra–ket notation may be intermixed with non-commuting operator
	expressions.
	
	Restrictions: For very long operator strings, the brute-force automatic
	reordering becomes slow, but it can be turned off. In such cases,
	the expectation values may still be evaluated using Wickʼs theorem.
	
	Unusual features: SNEG provides the natural notation of second-quantization
	operators (dagger for creation operators, etc.) when used interactively
	using the Mathematica notebook interface.
	
	Running time: Problem dependent},
  doi = {10.1016/j.cpc.2011.05.013},
  issn = {0010-4655},
  keywords = {Symbolic manipulation},
  url = {http://www.sciencedirect.com/science/article/pii/S0010465511001792}
}

@ARTICLE{Ryssel2010,
  author = {Uwe Ryssel and Joern Ploennigs and Klaus Kabitzsch},
  title = {Automatic library migration for the generation of hardware-in-the-loop
	models},
  journal = {Science of Computer Programming},
  year = {2010},
  pages = { - },
  number = {0},
  abstract = {Embedded systems are widely used in several applications nowadays.
	As they integrate hard- and software elements, their functionality
	and reliability are often tested by hardware-in-the-loop methods,
	in which the system under test runs in a simulated environment. Due
	to the rising complexity of the embedded functions, performance limitations
	and practicability reasons, the simulations are often specialized
	to test specific aspects of the embedded system and develop a high
	diversity by themselves. This diversity is difficult to manage for
	a user and results in erroneously selected test components and compatibility
	problems in the test configuration. This paper presents a generative
	programming approach that handles the diversity of test libraries.
	Compatibility issues are explicitly evaluated by a new interface
	concept. Furthermore, a novel model analyzer facilitates the efficient
	application in practice by migrating existing libraries. The approach
	is evaluated for an example from the automotive domain using MATLAB/Simulink.},
  doi = {10.1016/j.scico.2010.06.005},
  issn = {0167-6423},
  keywords = {Generative programming},
  url = {http://www.sciencedirect.com/science/article/pii/S0167642310001115}
}

@ARTICLE{DavidS2003209,
  author = {David S. and Wile},
  title = {Revealing component properties through architectural styles},
  journal = {Journal of Systems and Software},
  year = {2003},
  volume = {65},
  pages = {209 - 214},
  number = {3},
  note = {<ce:title>Component-Based Software Engineering</ce:title>},
  abstract = {An underlying assumption in even using the phrase “component certification
	and system prediction” is that an understanding of individual components’
	properties will lead to an understanding of a system’s properties
	by some form of compositional reasoning. Unfortunately, standard
	analytical composition techniques suffer from two problems: (1) they
	require that the internal structure of components be revealed in
	order to reason about them and (2) they deal clumsily with properties
	that require analysis of patterns of interaction. Here, based on
	the observation that a formal software architecture description itself
	is a constructive composition mechanism, I illustrate how the use
	of software architecture styles can sometimes alleviate the first
	problem and solve the latter.},
  doi = {10.1016/S0164-1212(02)00043-2},
  issn = {0164-1212},
  keywords = {Analytical composition},
  url = {http://www.sciencedirect.com/science/article/pii/S0164121202000432}
}

@ARTICLE{SadatMohtasham20081196,
  author = {S. Hossein Sadat-Mohtasham and Ali A. Ghorbani},
  title = {A language for high-level description of adaptive web systems},
  journal = {Journal of Systems and Software},
  year = {2008},
  volume = {81},
  pages = {1196 - 1217},
  number = {7},
  abstract = {Adaptive Web systems (AWS) are Web-based systems that can adapt their
	features such as, presentation, content, and structure, based on
	users’ behaviour and preferences, device capabilities, and environment
	attributes. A framework was developed in our research group to provide
	the necessary components and protocols for the development of adaptive
	Web systems; however, there were several issues and shortcomings
	(e.g. low productivity, lack of verification mechanisms, etc.) in
	using the framework that inspired the development of a domain-specific
	language for the framework. This paper focuses on the proposal, design,
	and implementation of AWL, the Adaptive Web Language, which is used
	to develop adaptive Web systems within our framework. Not only does
	AWL address the existing issues in the framework, but it also offers
	mechanisms to increase software quality attributes, especially, reusability.
	An example application named PENS (a personalized e-News system)
	is explained and implemented in AWL. AWL has been designed based
	on the analysis of the adaptive Web domain, having taken into account
	the principles of reuse-based software engineering (product-lines),
	domain-specific languages, and aspect-oriented programming. Specially,
	a novel design decision, inspired by aspect-oriented programming
	paradigm, allows separate specification of presentation features
	in an application from its adaptation features. The AWL’s design
	decisions and their benefits are explained.},
  doi = {10.1016/j.jss.2007.08.033},
  issn = {0164-1212},
  keywords = {Adaptive web system},
  url = {http://www.sciencedirect.com/science/article/pii/S0164121207002233}
}

@ARTICLE{Santos20101078,
  author = {André L. Santos and Kai Koskimies and Antónia Lopes},
  title = {Automating the construction of domain-specific modeling languages
	for object-oriented frameworks},
  journal = {Journal of Systems and Software},
  year = {2010},
  volume = {83},
  pages = {1078 - 1093},
  number = {7},
  note = {<ce:title>SPLC 2008</ce:title>},
  abstract = {The extension of frameworks with domain-specific modeling languages
	(DSML) has proved to be an effective way of improving the productivity
	in software product-line engineering. However, developing and evolving
	a DSML is typically a difficult and time-consuming task because it
	requires to develop and maintain a code generator, which transforms
	application models into framework-based code. In this paper, we propose
	a new approach for extending object-oriented frameworks that aims
	to alleviate this problem. The approach is based on developing an
	additional aspect-oriented layer that encodes a DSML for building
	framework-based applications, eliminating the need of implementing
	a code generator. We further show how a language workbench is capable
	of automating the construction of DSMLs using the proposed layer.},
  doi = {10.1016/j.jss.2010.01.047},
  issn = {0164-1212},
  keywords = {Domain-specific modeling},
  url = {http://www.sciencedirect.com/science/article/pii/S0164121210000312}
}

@ARTICLE{Sierra2008112,
  author = {José-Luis Sierra and Baltasar Fernández-Manjón and Alfredo Fernández-Valmayor},
  title = {A language-driven approach for the design of interactive applications},
  journal = {Interacting with Computers},
  year = {2008},
  volume = {20},
  pages = {112 - 127},
  number = {1},
  abstract = {In this paper we propose a language-driven approach for the high-level
	design of interactive applications architected according to the model-view-controller
	pattern. The approach is especially well-suited for applications
	that incorporate contents with sophisticated structures, and whose
	interactive behavior is driven by these structures. In our approach
	we characterize the structure of the contents stored in the applications’
	models with suitable domain-specific languages. Then we characterize
	the interactive behavior of these applications by assigning suitable
	operational semantics to these languages. The resulting designs are
	amenable to support rapid prototyping, exploration and early discovery
	of application features, systematic implementation using standard
	web-based technologies, and rational collaboration processes between
	domain experts and developers during production and maintenance.
	We exemplify the approach in the e-learning domain with a system
	for the production of Socratic tutors.},
  doi = {10.1016/j.intcom.2007.09.001},
  issn = {0953-5438},
  keywords = {Language-driven development},
  url = {http://www.sciencedirect.com/science/article/pii/S0953543807000768}
}

@ARTICLE{Sloane2010205,
  author = {Anthony M. Sloane and Lennart C.L. Kats and Eelco Visser},
  title = {A Pure Object-Oriented Embedding of Attribute Grammars},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2010},
  volume = {253},
  pages = {205 - 219},
  number = {7},
  note = {<ce:title>Proceedings of the Ninth Workshop on Language Descriptions
	Tools and Applications (LDTA 2009)</ce:title>},
  abstract = {Attribute grammars are a powerful specification paradigm for many
	language processing tasks, particularly semantic analysis of programming
	languages. Recent attribute grammar systems use dynamic scheduling
	algorithms to evaluate attributes by need. In this paper, we show
	how to remove the need for a generator, by embedding a dynamic approach
	in a modern, object-oriented programming language to implement a
	small, lightweight attribute grammar library. The Kiama attribution
	library has similar features to current generators, including cached,
	uncached, circular, higher-order and parameterised attributes, and
	implements new techniques for dynamic extension and variation of
	attribute equations. We use the Scala programming language because
	of its combination of object-oriented and functional features, support
	for domain-specific notations and emphasis on scalability. Unlike
	generators with specialised notation, Kiama attribute grammars use
	standard Scala notations such as pattern-matching functions for equations
	and mixins for composition. A performance analysis shows that our
	approach is practical for realistic language processing.},
  doi = {10.1016/j.entcs.2010.08.043},
  issn = {1571-0661},
  keywords = {language processing},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066110001222}
}

@ARTICLE{Soltenborn2011233,
  author = {Christian Soltenborn and Gregor Engels},
  title = {Using rule overriding to improve reusability and understandability
	of Dynamic Meta Modeling specifications},
  journal = {Journal of Visual Languages \&amp; Computing},
  year = {2011},
  volume = {22},
  pages = {233 - 250},
  number = {3},
  note = {<ce:title>Special Issue on Visual Analytics and Visual Semantics</ce:title>},
  abstract = {Dynamic Meta Modeling (DMM) is a visual semantics specification technique
	targeted at languages based on a metamodel. A DMM specification consists
	of a runtime metamodel and operational rules which describe how instances
	of the runtime metamodel change over time. A known deficiency of
	the DMM approach is that it does not support the refinement of a
	DMM specification, e.g., in the case of defining the semantics for
	a refined and extended domain-specific language (DSL). Up to now,
	DMM specifications could only be reused by adding or removing DMM
	rules.
	
	In this paper, we enhance DMM such that DMM rules can override other
	DMM rules, similar to a method being overridden in a subclass, and
	we show how rule overriding can be realized with the graph transformation
	tool GROOVE. We argue that rule overriding does not only have positive
	impact on reusability, but also improves the intuitive understandability
	of DMM semantics specifications.},
  doi = {10.1016/j.jvlc.2010.12.005},
  issn = {1045-926X},
  keywords = {Semantics},
  url = {http://www.sciencedirect.com/science/article/pii/S1045926X10000807}
}

@ARTICLE{Sprinkle2004291,
  author = {Jonathan Sprinkle and Gabor Karsai},
  title = {A domain-specific visual language for domain model evolution},
  journal = {Journal of Visual Languages \&amp; Computing},
  year = {2004},
  volume = {15},
  pages = {291 - 307},
  number = {3-4},
  note = {<ce:title>Domain-Specific Modeling with Visual Languages</ce:title>},
  abstract = {Domain-specific visual languages (DSVLs) are concise and useful tools
	that allow the rapid development of the behavior and/or structure
	of applications in well-defined domains. These languages are typically
	developed specifically for a domain, and have a strong cohesion to
	the domain concepts, which often appear as primitives in the language.
	The strong cohesion between DSVL language primitives and the domain
	is a benefit for development by domain experts, but can be a drawback
	when the domain evolves—even when that evolution appears to be
	insignificant. This paper presents a domain-specific visual language
	developed expressly for the evolution of domain-specific visual languages,
	and uses concepts from graph rewriting to specify and carry out the
	transformation of the models built using the original DSVL.},
  doi = {10.1016/j.jvlc.2004.01.006},
  issn = {1045-926X},
  keywords = {Domain-specific language},
  url = {http://www.sciencedirect.com/science/article/pii/S1045926X0400014X}
}

@ARTICLE{Svensson20102065,
  author = {Joel Svensson and Koen Claessen and Mary Sheeran},
  title = {GPGPU kernel implementation and refinement using Obsidian},
  journal = {Procedia Computer Science},
  year = {2010},
  volume = {1},
  pages = {2065 - 2074},
  number = {1},
  note = {<ce:title>ICCS 2010</ce:title>},
  abstract = {Obsidian is a domain specific language for data-parallel programming
	on graphics processors (GPUs). It is embedded in the functional programming
	language Haskell. The user writes code using constructs familiar
	from Haskell (like map and reduce), recursion and some specially
	designed combinators for combining GPU programs. NVIDIA CUDA code
	is generated from these high level descriptions, and passed to the
	nvcc compiler . Currently, we consider only the generation of single
	kernels, and not their coordination.
	
	This paper is focussed on how the user should work with Obsidian,
	starting with an obviously correct (or welltested) description of
	the required function, and refining it by the introduction of constructs
	to give finer control of the computation on the GPU. For some combinators,
	this approach results in CUDA code with satisfactory performance,
	promising increased productivity, as the high level descriptions
	are short and uncluttered. But for other combinators, the performance
	of generated code is not yet satisfactory. Ways to tackle this problem
	and plans to integrate Obsidian with another higher-level embedded
	language for GPU programming in Haskell are briefly discussed.},
  doi = {10.1016/j.procs.2010.04.231},
  issn = {1877-0509},
  keywords = {Data-parallel},
  url = {http://www.sciencedirect.com/science/article/pii/S1877050910002322}
}

@ARTICLE{Sánchez20111008,
  author = {Pedro Sánchez and Manuel Jiménez and Francisca Rosique and Bárbara
	Álvarez and Andrés Iborra},
  title = {A framework for developing home automation systems: From requirements
	to code},
  journal = {Journal of Systems and Software},
  year = {2011},
  volume = {84},
  pages = {1008 - 1021},
  number = {6},
  abstract = {This article presents an integrated framework for the development
	of home automation systems following the model-driven approach. By
	executing model transformations the environment allows developers
	to generate executable code for specific platforms. The tools presented
	in this work help developers to model home automation systems by
	means of a domain specific language which is later transformed into
	code for home automation specific platforms. These transformations
	have been defined by means of graph grammars and template engines
	extended with traceability capabilities. Our framework also allows
	the models to be reused for different applications since a catalogue
	of requirements is provided. This framework enables the development
	of home automation applications with techniques for improving the
	quality of both the process and the models obtained. In order to
	evaluate the benefits of the approach, we conducted a survey among
	developers that used the framework. The analysis of the outcome of
	this survey shows which conditions should be fulfilled in order to
	increase reusability.},
  doi = {10.1016/j.jss.2011.01.052},
  issn = {0164-1212},
  keywords = {Home automation},
  url = {http://www.sciencedirect.com/science/article/pii/S0164121211000422}
}

@ARTICLE{Tanrıöver2011448,
  author = {Ö. Özgür Tanrıöver and Semih Bilgen},
  title = {A framework for reviewing domain specific conceptual models},
  journal = {Computer Standards \&amp; Interfaces},
  year = {2011},
  volume = {33},
  pages = {448 - 464},
  number = {5},
  abstract = {Conceptual models are used in understanding and communicating the
	domain of interest during analysis phase of system development. As
	they are used in early phases, errors and omissions may propagate
	to later phases and may be very costly to correct. This paper proposes
	a framework for evaluating conceptual models when represented in
	a domain specific language based on UML constructs. The framework
	describes the main aspects to be considered when conceptual models
	are represented in a domain specific language, presents a classification
	of semantic issues and some evaluation indicators. The indicators
	can, in principle, identify situations in the models where inconsistencies
	or incompleteness might occur. Whether these are real concerns might
	depend on domain semantics, hence these are semantic, not syntactic
	checks. The use of the proposed review framework is illustrated in
	the context of two conceptual models in a domain specific notation,
	KAMA. With reviews based on the framework, it is possible to spot
	semantic issues which are not noticed by case tools and help the
	analyst to identify more information about the domain.},
  doi = {10.1016/j.csi.2010.12.001},
  issn = {0920-5489},
  keywords = {Conceptual model evaluation},
  url = {http://www.sciencedirect.com/science/article/pii/S0920548911000031}
}

@ARTICLE{Uwe2010733,
  author = {Uwe and Zdun},
  title = {A DSL toolkit for deferring architectural decisions in DSL-based
	software design},
  journal = {Information and Software Technology},
  year = {2010},
  volume = {52},
  pages = {733 - 748},
  number = {7},
  abstract = {A number of mature toolkits and language workbenches for DSL-based
	design have been proposed, making DSL-based design attractive for
	many projects. These toolkits preselect many architectural decision
	options. However, in many cases it would be beneficial for DSL-based
	design to decide for the DSL’s architecture later on in a DSL project,
	once the requirements and the domain have been sufficiently understood.
	We propose a language and a number of DSLs for DSL-based design and
	development that combine important benefits of different DSL toolkits
	in a unique way. Our approach specifically targets at deferring architectural
	decisions in DSL-based design. As a consequence, the architect can
	choose, even late in a DSL project, for options such as whether to
	provide the DSL as one or more external or embedded DSLs and whether
	to use an explicit language model or not .},
  doi = {10.1016/j.infsof.2010.03.004},
  issn = {0950-5849},
  keywords = {Software Architecture},
  url = {http://www.sciencedirect.com/science/article/pii/S0950584910000443}
}

@ARTICLE{Uzuner200813,
  author = {Özlem Uzuner and Tawanda C. Sibanda and Yuan Luo and Peter Szolovits},
  title = {A de-identifier for medical discharge summaries},
  journal = {Artificial Intelligence in Medicine},
  year = {2008},
  volume = {42},
  pages = {13 - 35},
  number = {1},
  abstract = {SummaryObjective Clinical records contain significant medical information
	that can be useful to researchers in various disciplines. However,
	these records also contain personal health information (PHI) whose
	presence limits the use of the records outside of hospitals.
	
	The goal of de-identification is to remove all PHI from clinical records.
	This is a challenging task because many records contain foreign and
	misspelled PHI; they also contain PHI that are ambiguous with non-PHI.
	These complications are compounded by the linguistic characteristics
	of clinical records. For example, medical discharge summaries, which
	are studied in this paper, are characterized by fragmented, incomplete
	utterances and domain-specific language; they cannot be fully processed
	by tools designed for lay language. Methods and results In this paper,
	we show that we can de-identify medical discharge summaries using
	a de-identifier, Stat De-id, based on support vector machines and
	local context (F-measure&#xa0;=&#xa0;97% on PHI). Our representation
	of local context aids de-identification even when PHI include out-of-vocabulary
	words and even when PHI are ambiguous with non-PHI within the same
	corpus. Comparison of Stat De-id with a rule-based approach shows
	that local context contributes more to de-identification than dictionaries
	combined with hand-tailored heuristics (F-measure&#xa0;=&#xa0;85%).
	Comparison with two well-known named entity recognition (NER) systems,
	SNoW (F-measure&#xa0;=&#xa0;94%) and IdentiFinder (F-measure&#xa0;=&#xa0;36%),
	on five representative corpora show that when the language of documents
	is fragmented, a system with a relatively thorough representation
	of local context can be a more effective de-identifier than systems
	that combine (relatively simpler) local context with global context.
	Comparison with a Conditional Random Field De-identifier (CRFD),
	which utilizes global context in addition to the local context of
	Stat De-id, confirms this finding (F-measure&#xa0;=&#xa0;88%) and
	establishes that strengthening the representation of local context
	may be more beneficial for de-identification than complementing local
	with global context.},
  doi = {10.1016/j.artmed.2007.10.001},
  issn = {0933-3657},
  keywords = {Automatic de-identification of narrative patient records},
  url = {http://www.sciencedirect.com/science/article/pii/S0933365707001327}
}

@ARTICLE{Vasudevan2011103,
  author = {Naveneetha Vasudevan and Laurence Tratt},
  title = {Comparative Study of DSL Tools},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2011},
  volume = {264},
  pages = {103 - 121},
  number = {5},
  note = {<ce:title>Proceedings of the Second Workshop on Generative Technologies
	(WGT) 2010</ce:title>},
  abstract = {An increasingly wide range of tools based on different approaches
	are being used to implement Domain Specific Languages (DSLs), yet
	there is little agreement as to which approach is, or approaches
	are, the most appropriate for any given problem. We believe this
	can in large part be explained by the lack of understanding within
	the DSL community. In this paper we aim to increase the understanding
	of the relative strengths and weaknesses of four approaches by implementing
	a common DSL case study. In addition, we present a comparative study
	of the four approaches.},
  doi = {10.1016/j.entcs.2011.06.007},
  issn = {1571-0661},
  keywords = {Domain Specific Languages},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066111000788}
}

@ARTICLE{Vern19992435,
  author = {Vern and Paxson},
  title = {Bro: a system for detecting network intruders in real-time},
  journal = {Computer Networks},
  year = {1999},
  volume = {31},
  pages = {2435 - 2463},
  number = {23-24},
  abstract = {We describe Bro, a stand-alone system for detecting network intruders
	in real-time by passively monitoring a network link over which the
	intruder's traffic transits. We give an overview of the system's
	design, which emphasizes high-speed (FDDI-rate) monitoring, real-time
	notification, clear separation between mechanism and policy, and
	extensibility. To achieve these ends, Bro is divided into an `event
	engine' that reduces a kernel-filtered network traffic stream into
	a series of higher-level events, and a `policy script interpreter'
	that interprets event handlers written in a specialized language
	used to express a site's security policy. Event handlers can update
	state information, synthesize new events, record information to disk,
	and generate real-time notifications via syslog. We also discuss
	a number of attacks that attempt to subvert passive monitoring systems
	and defenses against these, and give particulars of how Bro analyzes
	the six applications integrated into it so far: Finger, FTP, Portmapper,
	Ident, Telnet and Rlogin. The system is publicly available in source
	code form.},
  doi = {10.1016/S1389-1286(99)00112-7},
  issn = {1389-1286},
  keywords = {Network intrusion detection},
  url = {http://www.sciencedirect.com/science/article/pii/S1389128699001127}
}

@ARTICLE{Vespe2004427,
  author = {Vespe and Savikko},
  title = {Generative and incremental implementation for a scripting interface},
  journal = {Journal of Systems Architecture},
  year = {2004},
  volume = {50},
  pages = {427 - 439},
  number = {7},
  note = {<ce:title>Adaptable System/Software Architectures</ce:title>},
  abstract = {Many systems may benefit from scripting support, but the implementation
	of it is seldom trivial, especially if the system has not originally
	been developed with scripting support in mind. In this paper we describe
	a generative, incremental process for creating an intuitive Python
	interface to a large, hierarchic COM library. The approach is illuminated
	with the original, real-life case study.},
  doi = {10.1016/j.sysarc.2003.09.007},
  issn = {1383-7621},
  keywords = {Generative programming},
  url = {http://www.sciencedirect.com/science/article/pii/S1383762103001747}
}

@ARTICLE{WANG2011181,
  author = {Lize WANG and Bin LIU and Minyan LU},
  title = {A Modeling Language Based on UML for Modeling Simulation Testing
	System of Avionic Software},
  journal = {Chinese Journal of Aeronautics},
  year = {2011},
  volume = {24},
  pages = {181 - 194},
  number = {2},
  abstract = {With direct expression of individual application domain patterns and
	ideas, domain-specific modeling language (DSML) is more and more
	frequently used to build models instead of using a combination of
	one or more general constructs. Based on the profile mechanism of
	unified modeling language (UML) 2.2, a kind of DSML is presented
	to model simulation testing systems of avionic software (STSAS).
	To define the syntax, semantics and notions of the DSML, the domain
	model of the STSAS from which we generalize the domain concepts and
	relationships among these concepts is given, and then, the domain
	model is mapped into a UML meta-model, named UML-STSAS profile. Assuming
	a flight control system (FCS) as system under test (SUT), we design
	the relevant STSAS. The results indicate that extending UML to the
	simulation testing domain can effectively and precisely model STSAS.},
  doi = {10.1016/S1000-9361(11)60022-8},
  issn = {1000-9361},
  keywords = {avionics},
  url = {http://www.sciencedirect.com/science/article/pii/S1000936111600228}
}

@ARTICLE{Wolfgang200275,
  author = {Wolfgang and Polak},
  title = {Formal methods in practice},
  journal = {Science of Computer Programming},
  year = {2002},
  volume = {42},
  pages = {75 - 85},
  number = {1},
  note = {<ce:title>Special Issue on Engineering Automation for Computer Based
	Systems</ce:title>},
  abstract = {Technology transfer from academic research to industrial practice
	is hampered by social, political, and economic problems more than
	by technical issues. This paper describes one instance of successful
	technology transfer based on a special-purpose language and an associated
	translation tool tailored to the customer's needs. The key lesson
	to be learned from this example is that mathematical formalisms must
	be transparent to the user. Formalisms can be effectively employed
	if they are represented by tools that fit into existing work processes.
	It is suggested that special-purpose, domain-specific languages and
	their translators are an important vehicle to transition advanced
	technology to practice. This approach enables domain experts to solve
	problems using familiar terminology. It enables engineers of all
	disciplines to utilize computers without becoming software engineers.
	In doing so, we not only mitigate the chronic shortage of qualified
	software personnel but also simplify the problem of requirements
	analysis and specification.},
  doi = {10.1016/S0167-6423(01)00027-2},
  issn = {0167-6423},
  keywords = {Domain-specific languages},
  url = {http://www.sciencedirect.com/science/article/pii/S0167642301000272}
}

@ARTICLE{Wolfgang199962,
  author = {Wolfgang and Polak},
  title = {Formal Methods in Practice},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {1999},
  volume = {25},
  pages = {62 - 72},
  number = {0},
  note = {<ce:title>The 1998 ARO/ONR/NSF/DARPA Monterey Workshop on Engineering
	Automation for Computer Basesd Systems</ce:title>},
  abstract = {Technology transfer from academic research to industrial practice
	is hampered by social, political and economic problems more that
	by technical issues. This paper describes one instance of successful
	technology transfer based on a special-purpose language and associated
	translation tool tailored to the customer's needs. The key lesson
	to be learned from this example is that mathematical formalisms must
	be transparent to the user. Formalisms can be effectively employed
	if they are represented by tools that fit into existing work processes.
	
	It is suggested that the model of special-purpose, domain-specific
	languages and their translators are an important vehicle to transition
	advanced technology to practice. This approach enables domain experts
	to solve problems using familiar terminology. It enables engineers
	of all disciplines to utilize computers without becoming software
	engineers. In doing so we not only mitigate the chronic shortage
	of qualified software personnel but also simplify the problem of
	requirements analysis and specification.},
  doi = {10.1016/S1571-0661(04)00132-X},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S157106610400132X}
}

@ARTICLE{Wu201016,
  author = {Xiaoqing Wu and Barrett R. Bryant and Jeff Gray and Marjan Mernik},
  title = {Component-based LR parsing},
  journal = {Computer Languages, Systems \&amp; Structures},
  year = {2010},
  volume = {36},
  pages = {16 - 33},
  number = {1},
  abstract = {A language implementation with proper compositionality enables a compiler
	developer to divide-and-conquer the complexity of building a large
	language by constructing a set of smaller languages. Ideally, these
	small language implementations should be independent of each other
	such that they can be designed, implemented and debugged individually,
	and later be reused in different applications (e.g., building domain-specific
	languages). However, the language composition offered by several
	existing parser generators resides at the grammar level, which means
	all the grammar modules need to be composed together and all corresponding
	ambiguities have to be resolved before generating a single parser
	for the language. This produces tight coupling between grammar modules,
	which harms information hiding and affects independent development
	of language features. To address this problem, we have developed
	a novel parsing algorithm that we call Component-based LR (CLR) parsing,
	which provides code-level compositionality for language development
	by producing a separate parser for each grammar component. In addition
	to shift and reduce actions, the algorithm extends general LR parsing
	by introducing switch and return actions to empower the parsing action
	to jump from one parser to another. Our experimental evaluation demonstrates
	that CLR increases the comprehensibility, reusability, changeability
	and independent development ability of the language implementation.
	Moreover, the loose coupling among parser components enables CLR
	to describe grammars that contain LR parsing conflicts or require
	ambiguous token definitions, such as island grammars and embedded
	languages.},
  doi = {10.1016/j.cl.2009.01.002},
  issn = {1477-8424},
  keywords = {Component-based software development},
  url = {http://www.sciencedirect.com/science/article/pii/S1477842409000037}
}

@ARTICLE{VanWyk201039,
  author = {Eric Van Wyk and Derek Bodin and Jimin Gao and Lijesh Krishnan},
  title = {Silver: An extensible attribute grammar system},
  journal = {Science of Computer Programming},
  year = {2010},
  volume = {75},
  pages = {39 - 54},
  number = {1-2},
  note = {<ce:title>Special Issue on ETAPS 2006 and 2007 Workshops on Language
	Descriptions, Tools, and Applications (LDTA ’06 and ’07)</ce:title>},
  abstract = {Attribute grammar specification languages, like many domain-specific
	languages, offer significant advantages to their users, such as high-level
	declarative constructs and domain-specific analyses. Despite these
	advantages, attribute grammars are often not adopted to the degree
	that their proponents envision. One practical obstacle to their adoption
	is a perceived lack of both domain-specific and general purpose language
	features needed to address the many different aspects of a problem.
	Here we describe Silver, an extensible attribute grammar specification
	system, and show how it can be extended with general purpose features
	such as pattern matching and domain-specific features such as collection
	attributes and constructs for supporting data-flow analysis of imperative
	programs. The result is an attribute grammar specification language
	with a rich set of language features. Silver is implemented in itself
	by a Silver attribute grammar and utilizes forwarding to implement
	the extensions in a cost-effective manner.},
  doi = {10.1016/j.scico.2009.07.004},
  issn = {0167-6423},
  keywords = {Extensible languages},
  url = {http://www.sciencedirect.com/science/article/pii/S0167642309001099}
}

@ARTICLE{VanWyk2008103,
  author = {Eric Van Wyk and Derek Bodin and Jimin Gao and Lijesh Krishnan},
  title = {Silver: an Extensible Attribute Grammar System},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2008},
  volume = {203},
  pages = {103 - 116},
  number = {2},
  note = {<ce:title>Proceedings of the Seventh Workshop on Language Descriptions,
	Tools, and Applications (LDTA 2007)</ce:title>},
  abstract = {Attribute grammar specification languages, like many domain specific
	languages, offer significant advantages to their users, such as high-level
	declarative constructs and domain-specific analyses. Despite these
	advantages, attribute grammars are often not adopted to the degree
	that their proponents envision. One practical obstacle to their adoption
	is a perceived lack of both domain-specific and general purpose language
	features needed to address the many different aspects of a problem.
	Here we describe Silver, an extensible attribute grammar specification
	language, and show how it can be extended with general purpose features
	such as pattern matching and domain specific features such as collection
	attributes and constructs for supporting data-flow analysis of imperative
	programs. The result is an attribute grammar specification language
	with a rich set of language features. Silver is implemented in itself
	by a Silver attribute grammar and utilizes forwarding to implement
	the extensions in a cost-effective manner.},
  doi = {10.1016/j.entcs.2008.03.047},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066108001527}
}

@ARTICLE{Zeng2006103,
  author = {Jia Zeng and Chuck Mitchell and Stephen A. Edwards},
  title = {A Domain-Specific Language for Generating Dataflow Analyzers},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = {2006},
  volume = {164},
  pages = {103 - 119},
  number = {2},
  note = {<ce:title>Proceedings of the Sixth Workshop on Language Descriptions,
	Tools, and Applications (LDTA 2006)</ce:title> <xocs:full-name>Sixth
	Workshop on Language Descriptions, Tools, and Applications</xocs:full-name>},
  abstract = {Dataflow analysis is a well-understood and very powerful technique
	for analyzing programs as part of the compilation process. Virtually
	all compilers use some sort of dataflow analysis as part of their
	optimization phase. However, despite being well-understood theoretically,
	such analyses are often difficult to code, making it difficult to
	quickly experiment with variants.
	
	To address this, we developed a domain-specific language, Analyzer
	Generator (AG), that synthesizes dataflow analysis phases for Microsoft's
	Phoenix compiler framework. AG hides the fussy details needed to
	make analyses modular, yet generates code that is as efficient as
	the hand-coded equivalent. One key construct we introduce allows
	IR object classes to be extended without recompiling.
	
	Experimental results on three analyses show that AG code can be one-tenth
	the size of the equivalent handwritten C++ code with no loss of performance.
	It is our hope that AG will make developing new dataflow analyses
	much easier.},
  doi = {10.1016/j.entcs.2006.10.008},
  issn = {1571-0661},
  keywords = {Domain-specific language},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066106004853}
}

@ARTICLE{Zeuch2011629,
  author = {Nina Zeuch and Heinz Holling and Jörg-Tobias Kuhn},
  title = {Analysis of the Latin Square Task with linear logistic test models},
  journal = {Learning and Individual Differences},
  year = {2011},
  volume = {21},
  pages = {629 - 632},
  number = {5},
  abstract = {The Latin Square Task (LST) was developed by Birney, Halford, and
	Andrews [Birney, D. P., Halford, G. S., &amp; Andrews, G. (2006).
	Measuring the influence of cognitive complexity on relational reasoning:
	The development of the Latin Square Task. Educational and Psychological
	Measurement, 66, 146–171.] and represents a non-domain specific,
	language-free operationalization of Relational Complexity (RC-)Theory.
	The current study investigates the basic cognitive parameters and
	structure of LST as defined by RC-Theory, using IRT-based linear
	logistic test models (LLTM). 850 German school students completed
	26 systematically designed LST items. Results support the notion
	of Rasch-scalability. LLTM analyses reveal that both operation complexity
	and number of operations affect item difficulty. It is shown how
	LLTM and its variants can provide substantial insights into cognitive
	solution processes and composition of item difficulty in relational
	reasoning in order to make item construction more efficient.},
  doi = {10.1016/j.lindif.2011.03.004},
  issn = {1041-6080},
  keywords = {Latin square task},
  url = {http://www.sciencedirect.com/science/article/pii/S1041608011000392}
}

@ARTICLE{Zhang2006209,
  author = {Hongyu Zhang and Jeremy S. Bradbury and James R. Cordy and Juergen
	Dingel},
  title = {Using source transformation to test and model check implicit-invocation
	systems},
  journal = {Science of Computer Programming},
  year = {2006},
  volume = {62},
  pages = {209 - 227},
  number = {3},
  note = {<ce:title>Special issue on Source code analysis and manipulation
	(SCAM 2005)</ce:title>},
  abstract = {In this paper we present a source transformation-based framework to
	support uniform testing and model checking of implicit-invocation
	software systems. The framework includes a new domain-specific programming
	language, the Implicit-Invocation Language (IIL), explicitly designed
	for directly expressing implicit-invocation software systems, and
	a set of formal rule-based source transformation tools that allow
	automatic generation of both executable and formal verification artifacts.
	We provide details of these transformation tools, evaluate the framework
	in practice, and discuss the benefits of formal automatic transformation
	in this context. Our approach is designed not only to advance the
	state-of-the-art in validating implicit-invocation systems, but also
	to further explore the use of automated source transformation as
	a uniform vehicle to assist in the implementation, validation and
	verification of programming languages and software systems in general.},
  doi = {10.1016/j.scico.2006.04.008},
  issn = {0167-6423},
  keywords = {Source transformation},
  url = {http://www.sciencedirect.com/science/article/pii/S0167642306000955}
}

@ARTICLE{Zhu20071390,
  author = {Nianping Zhu and John Grundy and John Hosking and Na Liu and Shuping
	Cao and Akhil Mehra},
  title = {Pounamu: A meta-tool for exploratory domain-specific visual language
	tool development},
  journal = {Journal of Systems and Software},
  year = {2007},
  volume = {80},
  pages = {1390 - 1407},
  number = {8},
  note = {<ce:title>The Impact of Barry Boehm’s Work on Software Engineering
	Education and Training</ce:title>},
  abstract = {Domain-specific visual language tools have become important in many
	domains of software engineering and end user development. However
	building such tools is very challenging with a need for multiple
	views of information and multi-user support, the ability for users
	to change tool diagram and meta-model specifications while in use,
	and a need for an open architecture for tool integration. We describe
	Pounamu, a meta-tool for realising such visual design environments.
	We describe the motivation for Pounamu, its architecture and implementation
	and illustrate examples of domain-specific visual language tools
	that we have developed with Pounamu.},
  doi = {10.1016/j.jss.2006.10.028},
  issn = {0164-1212},
  keywords = {Meta-tools},
  url = {http://www.sciencedirect.com/science/article/pii/S0164121206002871}
}

